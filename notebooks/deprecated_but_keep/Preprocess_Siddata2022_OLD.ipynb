{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e497cf-40ce-4001-b2bd-48a0244d889f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from Levenshtein import distance\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from os.path import join, isdir, isfile, abspath, dirname, splitext, basename, split\n",
    "from derive_conceptualspace.util.mpl_tools import show_hist\n",
    "from collections import Counter\n",
    "\n",
    "BOOK_BASE = \"/home/chris/Documents/UNI_neu/Masterarbeit/OTHER/study_behavior_analysis/src/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f38f9-ae5c-4ffe-8bd9-68c0fd6dcd2e",
   "metadata": {},
   "source": [
    "# Looking at the DDC-Classification of the course Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0389d-1182-41ca-952a-c080bb682d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/chris/Documents/UNI_neu/Masterarbeit/OTHER/study_behavior_analysis/EducationalResource-2022-01-20.csv\"\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "assert all(i.startswith('\"') and i.endswith('\"') and i[1:-1].isnumeric() for i in df[\"ddc_code\"] if not pd.isna(i))\n",
    "df[\"ddc_code\"] = df[\"ddc_code\"].str[1:-1]#.astype(pd.Int64Dtype())\n",
    "df = df.drop(columns=[\"identifier\", \"contributor\", \"creator\", \"coverage\", \"date\", \"rights\", \"relation\"])\n",
    "df[\"description\"] = df[\"description\"].str.strip().str.replace(\"\\r\\n\", \"\\n\") \n",
    "df[\"description\"] = df[\"description\"].str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\")\n",
    "df[\"description\"] = df[\"description\"].str.replace(\"\\n\", \" \") \n",
    "df[\"description\"] = df[\"description\"].str.strip()\n",
    "df[\"title\"] = df[\"title\"].str.strip()\n",
    "display(df.describe())\n",
    "print(\"\\n\\n\")\n",
    "display(df.head())\n",
    "print(\"\\n\\n\")\n",
    "display(df[\"origin\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca06a3-d7eb-41c3-80e5-790fb571e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"type\"].unique())\n",
    "df[\"type\"] = df[\"type\"].str.replace(\"['SIP']\", \"SIP\", regex=False).str.replace(\"['udemy', 'mooc']\", \"udemy_mooc\", regex=False).str.replace(\"['OER']\", \"OER\", regex=False)\n",
    "print(df[\"format\"].unique())\n",
    "df[\"format\"] = df[\"format\"].str.replace(\"['CRS']\", \"CRS\", regex=False)\n",
    "df = df.set_index(\"id\")\n",
    "df = df.dropna(subset=[\"title\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0006612e-dcdd-4e01-8dd0-e681063e41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df[\"description\"].str.count(\" \") >= 9]\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b4c56-469d-486f-8c11-351a3912618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df[\"publisher\"].value_counts())\n",
    "# seldom_publishers = [k for k,v in df[\"publisher\"].value_counts().items() if v <= 20]\n",
    "# display(df[df[\"publisher\"].isin(seldom_publishers)])\n",
    "# print(df[df[\"publisher\"].isin(seldom_publishers)][\"relation\"][1129]) #<- links sind tot\n",
    "# print(df[df[\"publisher\"].isin(seldom_publishers)][\"source\"][1129])   #<- links sind tot\n",
    "\n",
    "# #wir sehen, die sind alle of type OER. \n",
    "# #Interessante Frage wäre noch was Relation ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a532138-9223-41ea-863c-afb790cb9c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[~df[\"publisher\"].isin(seldom_publishers)]\n",
    "# df = df.set_index(\"id\")\n",
    "# df = df.drop(columns=[\"relation\", \"source\"])\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2e19a-ee0e-4409-a3a3-6e13273b2b86",
   "metadata": {},
   "source": [
    "## Looking at course data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f2c3a-7fdd-476d-8ddd-70c34956a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(join(BOOK_BASE, \"data/course_data/db_dump_new\", \"institute_dump.csv\"))\n",
    "# print(len(df))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c3aad2-ac94-42fa-8d3f-dccffc486337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(join(BOOK_BASE, \"data/course_data/db_dump_new\", \"lecturer_dump.csv\"))\n",
    "# print(len(df))\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6efed-de39-4784-b042-102e0a672b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f59b6a-754f-451a-8502-312aaa54c92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdn = pd.read_csv(join(BOOK_BASE, \"data/course_data/db_dump_new\", \"course_dump_new.csv\"))\n",
    "print(len(cdn))\n",
    "cdn = cdn.drop(columns=[\"course_origin_id\", \"place\", \"start_time\", \"end_time\", \"end_semester\", \"date\", \"TF_IDF_scores\", \"origin_id\"])\n",
    "#display(cdn.head())\n",
    "assert all(i.startswith('\"\\\\\"') and i.endswith('\\\\\"\"') and i[3:-3].isnumeric() for i in cdn[\"ddc_code\"] if not pd.isna(i))\n",
    "cdn[\"ddc_code\"] = cdn[\"ddc_code\"].str[3:-3]\n",
    "cdn = cdn.set_index(\"id\")\n",
    "cdn = cdn.dropna(subset=[\"title\"])\n",
    "cdn[\"description\"] = cdn[\"description\"].str.strip().str.replace(\"\\r\\n\", \"\\n\")\n",
    "cdn[\"description\"] = cdn[\"description\"].str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\")\n",
    "cdn[\"description\"] = cdn[\"description\"].str.replace(\"\\n\", \" \")\n",
    "cdn[\"description\"] = cdn[\"description\"].str.strip()\n",
    "cdn[\"title\"] = cdn[\"title\"].str.strip()\n",
    "cdn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6080557d-d113-48d3-9eb2-c153d36986f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "edu = pd.read_csv(join(BOOK_BASE, \"data/course_data/db_dump_new\", \"eduresource_dump.csv\"))\n",
    "print(len(edu))\n",
    "print(\"#Entries for 'contributor' column:\", len(edu[(~edu[\"contributor\"].isna()) & (~edu[\"contributor\"].isin([[], \"[]\"]))]))\n",
    "print(\"#Entries for 'creator' column:\", len(edu[(~edu[\"creator\"].isna()) & (~edu[\"creator\"].isin([[], \"[]\"]))]))\n",
    "# edu.head()\n",
    "edu = edu.drop(columns=[\"TF_IDF_scores\", \"contributor\", \"date\", \"relation\", \"rights\", \"origin_id\"])\n",
    "display(edu[\"format\"].unique())\n",
    "edu[\"format\"] = edu[\"format\"].str.replace('[\"udemy\", \"mooc\"]', \"udemy_mooc\", regex=False).str.replace('[\"CRS\"]', \"CRS\", regex=False).str.replace('\"video/mp4\"', \"mp4\", regex=False).str.replace('[]', \"\", regex=False)\n",
    "display(edu[\"type\"].unique())\n",
    "edu[\"type\"] = edu[\"type\"].str.replace('[\"SIP\"]', \"SIP\", regex=False).str.replace('[\"udemy\", \"mooc\"]', \"udemy_mooc\", regex=False).str.replace('[\"video\", \"OER\"]', \"video_OER\", regex=False).str.replace('[\"WEB\"]', \"web\", regex=False)\n",
    "display(edu[\"type\"].unique())\n",
    "edu = edu.set_index(\"id\")\n",
    "assert all(i.startswith('\"\\\\\"') and i.endswith('\\\\\"\"') and i[3:-3].isnumeric() for i in edu[\"ddc_code\"] if not pd.isna(i))\n",
    "edu[\"ddc_code\"] = edu[\"ddc_code\"].str[3:-3]\n",
    "edu = edu.dropna(subset=[\"title\"])\n",
    "edu[\"description\"] = edu[\"description\"].str.strip().str.replace(\"\\r\\n\", \"\\n\")\n",
    "edu[\"description\"] = edu[\"description\"].str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\")\n",
    "edu[\"description\"] = edu[\"description\"].str.replace(\"\\n\", \" \")\n",
    "edu[\"description\"] = edu[\"description\"].str.strip()\n",
    "edu[\"title\"] = edu[\"title\"].str.strip()\n",
    "edu.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc7bd9-3d69-4032-894c-65ac6d4df0af",
   "metadata": {},
   "source": [
    "## Merging them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f2d8a-1b6a-4a9c-9410-1629732e34b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(cdn))\n",
    "print(len(edu))\n",
    "display(df.head(2))\n",
    "display(cdn.head(2))\n",
    "display(edu.head(2))\n",
    "print(list(df.dtypes))\n",
    "print(list(cdn.dtypes))\n",
    "print(list(edu.dtypes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9539a51d-c41f-42ec-b299-d9763b3baf48",
   "metadata": {},
   "source": [
    "### merging. First the dumps `df` and `cdn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95b19b-7ae0-4f09-a18d-6623300c50cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdn[\"type\"] = \"SIP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cef0f-ba23-41d4-9400-2375b341fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(df[\"title\"])), len(set(cdn[\"title\"])), len(set(df[\"title\"])&set(cdn[\"title\"])), len(set(df[\"title\"])|set(cdn[\"title\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23982872-878a-47d4-8a1e-fee92f3f36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"] = df[\"description\"].str.replace(\"\\n\", \". \")\n",
    "cdn[\"description\"] = cdn[\"description\"].str.replace(\"\\n\", \". \")\n",
    "tmp = df[df[\"type\"] == \"SIP\"].merge(cdn, on=\"title\", how=\"left\", suffixes=(\"_df\", \"_cdn\"))\n",
    "#TODO tmp += df[df[\"type\"] != \"SIP\"]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd9ad4-36c2-43d7-b93e-46743b0a1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp.drop(columns=[\"origin\", \"type_df\", \"type_cdn\"])\n",
    "tmp[\"type\"] = \"SIP\"\n",
    "print(\"Courses that have a different DDC in the two joined dataframes:\")\n",
    "display(tmp[(~tmp[\"ddc_code_df\"].isna()) & (~tmp[\"ddc_code_cdn\"].isna()) & (tmp[\"ddc_code_df\"] != tmp[\"ddc_code_cdn\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f61fa8b-ad4c-4595-939c-d8489fba7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[\"ddc_code\"] = [i[1][\"ddc_code_cdn\"] if not pd.isna(i[1][\"ddc_code_cdn\"]) else i[1][\"ddc_code_df\"] for i in tmp.iterrows()] #so this may lead to a wrong ddc for TWO samples, that's okay.\n",
    "tmp = tmp.drop(columns=[\"ddc_code_cdn\", \"ddc_code_df\"])\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabdaa54-7e4b-4680-a395-44697f97a135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'equal descriptions for both: {len(tmp[tmp[\"description_df\"]==tmp[\"description_cdn\"]])}')\n",
    "#display(tmp[tmp[\"description_df\"]!=tmp[\"description_cdn\"]][[\"title\", \"description_df\", \"description_cdn\"]])\n",
    "different_descriptions_mask = (((~tmp[\"description_df\"].isna())&(~tmp[\"description_cdn\"].isna())) & (tmp[\"description_df\"].str.lower()!=tmp[\"description_cdn\"].str.lower()))\n",
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', 20, 'display.expand_frame_repr', False, 'display.max_colwidth', 10000, 'display.float_format', '{:.4f}'.format):\n",
    "    display(tmp[different_descriptions_mask][[\"title\", \"description_df\", \"description_cdn\"]])\n",
    "different_descriptions = tmp[different_descriptions_mask][[\"title\", \"description_df\", \"description_cdn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d892bd3-cd83-4e75-8900-f67f24219297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Left to do: \n",
    "# * tmp += df[df[\"type\"] != \"SIP\"] (add the non-courses from df)\n",
    "# * das koomplette `edu` ebenfalls reinmergen\n",
    "# * die 8432 with equal descriptions einfach die eine übernehmen\n",
    "# * noch nen bisschen smarter mergen (bei einigen ist nur die Jahreszahl anders, bei einigen steht halt nur \"online\" dabei, etc) (sent_tokenize und einfach jeden satz 1 mal übernehmen?)\n",
    "# * nachdem edu reingemergt ist die mit komplett leeren descriptions löschen\n",
    "# * bei zeilen mit gleichen titeln auch die descriptions concatenaten\n",
    "# * merge with the siddata2021 dataset at least because I need the course number from there\n",
    "# * doch die origin-column behalten, vielleicht erkenn ich da die originalen siddata2021 kurse dran\n",
    "# * merged_all[\"coverage\"] muss an die finale description angehängt werden\n",
    "# --> mit den description_df und description_cdn einfach genauso umgehen und die zu einer spalten machen und dann zeilenweise den sent_tokenize kram machen\n",
    "# Also: 1) die non-courses von df reinmergen 2) edu reinmergen, 3) da wo description_df = description_cdn (oder != description_edu) extra zeilen draus machen 4) dafür dann sent_tokenize etc\n",
    "#PS: tried other way round but this makes more sense: FIRST make separate rows for where description_cdn and description_df differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d8670-579d-47cf-9637-9209a6dc12f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tmp.reindex(sorted(tmp.columns), axis=1)\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0015232f-b9aa-4f25-8d9b-2d5cd36edc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonsip = df[df[\"type\"] != \"SIP\"].rename(columns={\"description\": \"description_df\"})\n",
    "nonsip = nonsip.reset_index().drop(columns=[\"id\", \"origin\"])\n",
    "nonsip[\"description_cdn\"] = pd.NA\n",
    "nonsip[\"start_semester\"] = pd.NA\n",
    "nonsip[\"url\"] = pd.NA\n",
    "nonsip = nonsip.reindex(sorted(nonsip.columns), axis=1)\n",
    "print(\"tmp-len\", len(tmp), \"nonsip-len\", len(nonsip))\n",
    "tmp = pd.concat([tmp, nonsip])\n",
    "print(\"merged-len\", len(tmp))\n",
    "display(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c03b22-6551-42a7-ba4a-0e7c64037ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_merged = tmp[((tmp[\"description_cdn\"].isna())^(tmp[\"description_df\"].isna()))]  #ok so first only those with only one description\n",
    "print(len(new_merged))\n",
    "new_merged = pd.concat([new_merged, tmp[tmp[\"description_cdn\"]==tmp[\"description_df\"]]]) #then those where the description is equal\n",
    "print(len(new_merged)) \n",
    "new_merged = pd.concat([new_merged, tmp[(tmp[\"description_cdn\"].isna()) & (tmp[\"description_df\"].isna())]]) #then those who don't have any description at all\n",
    "print(len(new_merged)) \n",
    "#thing that's left: those where there is a description for both and it's different\n",
    "different_desc_mask = (~tmp[\"description_cdn\"].isna()) & (~tmp[\"description_df\"].isna()) & (tmp[\"description_cdn\"]!=tmp[\"description_df\"])\n",
    "assert len(new_merged)+len(tmp[different_desc_mask]) == len(tmp) #ensure these 4 types are actually all\n",
    "new_merged[\"description\"] = [i[\"description_cdn\"] if not pd.isna(i[\"description_cdn\"]) else i[\"description_df\"] for num, i in new_merged.iterrows()]\n",
    "new_merged = new_merged.drop(columns=[\"description_cdn\", \"description_df\"])\n",
    "new_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d4aa5-2b82-4d3f-ada0-a62dc8f7ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(tmp[different_desc_mask].head())\n",
    "doublings = tmp[different_desc_mask]\n",
    "doublings[\"description\"] = [[row[\"description_cdn\"], row[\"description_df\"]] for num, row in doublings.iterrows()]\n",
    "doublings = doublings.drop(columns=[\"description_cdn\", \"description_df\"])\n",
    "doublings = doublings.explode(\"description\")\n",
    "\n",
    "new_merged = pd.concat([new_merged, doublings])\n",
    "new_merged = new_merged.reindex(sorted(new_merged.columns), axis=1)\n",
    "print(len(new_merged[new_merged[\"description\"].isna()]))\n",
    "new_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dfb4b3-65bb-40a7-927d-d8dd39673a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Left to do: \n",
    "# * das koomplette `edu` ebenfalls reinmergen und exploden\n",
    "# * nachdem edu reingemergt ist die mit komplett leeren descriptions löschen\n",
    "# * danach grouped-by-title die descriptions smart mergen\n",
    "#     * bei einigen ist nur die Jahreszahl anders, bei einigen steht halt nur \"online\" dabei, etc \n",
    "#     * sent_tokenize und einfach jeden satz 1 mal übernehmen?\n",
    "#     * auf jeden Fall ungleiches concatenaten, und dann pro kurs die maximale nichtdoppelte beschreibung haben\n",
    "# * merge with the siddata2021 dataset at least because I need the course number from there\n",
    "#    * doch die origin-column behalten, vielleicht erkenn ich da die originalen siddata2021 kurse dran\n",
    "# * new_merged[\"coverage\"] muss an die finale description angehängt werden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21fc59d-2b2b-47af-aebb-8219203eecf7",
   "metadata": {},
   "source": [
    "#### merging with edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddcd80-e141-4799-9dfa-83fc6c23f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "edu = edu.reindex(sorted(edu.columns), axis=1)\n",
    "edu = edu.reset_index().drop(columns=[\"id\"])\n",
    "display(edu.head())\n",
    "print(len(edu))\n",
    "display(new_merged.head())\n",
    "print(len(new_merged))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c8311-e315-4098-9d10-1d6b35e49610",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all = new_merged.merge(edu, on=\"title\", how=\"left\", suffixes=(\"_dfcdn\", \"_edu\"))\n",
    "#merged_all = merged_all.rename(columns={\"description\": \"description_edu\"})\n",
    "merged_all = merged_all.reindex(sorted(merged_all.columns), axis=1)\n",
    "print(len(merged_all))\n",
    "merged_all = merged_all.drop(columns=[\"identifier\"])\n",
    "merged_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dec6d9-8405-4c72-a2f6-7f2974014f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now I can delete those that don't have any description in either of the 3 csvs\n",
    "merged_all = merged_all[((~merged_all[\"description_dfcdn\"].isna())|(~merged_all[\"description_edu\"].isna()))]\n",
    "print(len(merged_all))\n",
    "\n",
    "merged_all[\"description_dfcdn\"] = merged_all[\"description_dfcdn\"].str.replace(r'<.*?>', '', regex=True)\n",
    "merged_all[\"description_edu\"] = merged_all[\"description_edu\"].str.replace(r'<.*?>', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea045e-5e72-4556-82c8-3dc3f4483d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now they may still differ in description, but also in type, coverage, creator, format, ddc ...\n",
    "merged_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4893b73-baaf-44a3-9bac-4754d1db07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_all = merged_all[((merged_all[\"description_dfcdn\"].isna())^(merged_all[\"description_edu\"].isna()))]  #ok so first only those with only one description\n",
    "print(len(new_all))\n",
    "new_all = pd.concat([new_all, merged_all[merged_all[\"description_dfcdn\"]==merged_all[\"description_edu\"]]]) #then those where the description is equal\n",
    "print(len(new_all)) \n",
    "#thing that's left: those where there is a description for both and it's different\n",
    "different_desc_mask = (~merged_all[\"description_dfcdn\"].isna()) & (~merged_all[\"description_edu\"].isna()) & (merged_all[\"description_dfcdn\"]!=merged_all[\"description_edu\"])\n",
    "assert len(new_all)+len(merged_all[different_desc_mask]) == len(merged_all) #ensure these 3 types are actually all (there are no full NA-rows anymore)\n",
    "#now we need to take the other information from either edu or dfdcn, depending on where it came from\n",
    "for col in [\"ddc_code\", \"format\", \"language\", \"publisher\", \"source\", \"type\"]:\n",
    "    print(col)\n",
    "    with pd.option_context('display.max_rows', 5, 'display.max_columns', 25, 'display.expand_frame_repr', False, 'display.max_colwidth', 20, 'display.float_format', '{:.4f}'.format):\n",
    "        display(new_all[(new_all[f\"{col}_dfcdn\"] != new_all[f\"{col}_edu\"]) & (~new_all[f\"{col}_dfcdn\"].isna()) & (~new_all[f\"{col}_edu\"].isna())])\n",
    "    #new_all[col] = [i[f\"{col}_dfcdn\"] if not pd.isna(i[\"description_dfcdn\"]) else i[f\"{col}_edu\"] for num, i in new_all.iterrows()]\n",
    "    #new_all = new_all.drop(columns=[f\"{col}_dfcdn\", f\"{col}_edu\"])\n",
    "# with pd.option_context('display.max_rows', 5, 'display.max_columns', 25, 'display.expand_frame_repr', False, 'display.max_colwidth', 20, 'display.float_format', '{:.4f}'.format):\n",
    "#     display(new_all.head())\n",
    "#new_all[\"description\"] = [i[\"description_dfcdn\"] if not pd.isna(i[\"description_dfcdn\"]) else i[\"description_edu\"] for num, i in new_all.iterrows()]\n",
    "#new_all = new_all.drop(columns=[\"description_dfcdn\", \"description_edu\"])\n",
    "#new_all = new_all.reindex(sorted(new_all.columns), axis=1)\n",
    "#new_all.head()\n",
    "\n",
    "# You know what, maybe I just explode them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32c291-7adf-49e9-8780-1bcfb6071b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, I will make one row per description and for the other columns (ddc_code, language, ...) I'll just concatenate dfcdn and edu, and then I'll groupby names and concatenate EVERYTHING\n",
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', 25, 'display.expand_frame_repr', False, 'display.max_colwidth', 20, 'display.float_format', '{:.4f}'.format):\n",
    "    display(merged_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1298229b-5969-42cf-b22f-620e13580a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"ddc_code\", \"format\", \"language\", \"publisher\", \"source\", \"type\", \"subject\"]:\n",
    "    merged_all[col] = [tuple([row[f\"{col}_dfcdn\"], row[f\"{col}_edu\"]]) for num, row in merged_all.iterrows()]\n",
    "    merged_all = merged_all.drop(columns=[f\"{col}_dfcdn\", f\"{col}_edu\"])\n",
    "    \n",
    "merged_all = merged_all.reindex(sorted(merged_all.columns), axis=1)\n",
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', 25, 'display.expand_frame_repr', False, 'display.max_colwidth', 20, 'display.float_format', '{:.4f}'.format):\n",
    "    display(merged_all)\n",
    "#okay, and now I'll explode the descriptions, and everything from then on I can do grouped by title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b985bdfa-ef28-4cae-8ded-70a77c72e630",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all[\"description\"] = [[row[f\"description_dfcdn\"], row[f\"description_edu\"]] for num, row in merged_all.iterrows()]\n",
    "merged_all = merged_all.drop(columns=[f\"description_dfcdn\", f\"description_edu\"])\n",
    "merged_all = merged_all.explode(\"description\")\n",
    "merged_all = merged_all.drop_duplicates()\n",
    "merged_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f48570-d999-4d05-9505-c499ea3f174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So now I want to group-by-title and add as much information as possible. For that:\n",
    "# * coverage an die description anhängen wenn vorhanden\n",
    "# * die mit komplett leeren descriptions löschen\n",
    "# * smart mergen:\n",
    "#     * bei einigen ist nur die Jahreszahl anders, bei einigen steht halt nur \"online\" dabei, etc \n",
    "#     * sent_tokenize und einfach jeden satz 1 mal übernehmen?\n",
    "#     * auf jeden Fall ungleiches concatenaten, und dann pro kurs die maximale nichtdoppelte beschreibung haben\n",
    "# * merge with the siddata2021 dataset at least because I need the course number from there\n",
    "#    * doch die origin-column behalten, vielleicht erkenn ich da die originalen siddata2021 kurse dran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c219005-a058-45f9-8615-9c22ae55eb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all[merged_all[\"coverage\"]==\"Wed 30.0920 \"] = pd.NA\n",
    "merged_all[\"coverage\"] = merged_all[\"coverage\"].str.replace(\"Wed 30.0920 \", \"\").str.replace(\"Wed 31.0321 \", \"\")\n",
    "merged_all[\"creator_name\"] = [[j.get(\"title\") for j in json.loads(i) if isinstance(j, dict)]  if isinstance(i, str) and i != \"{}\" else pd.NA for i in merged_all[\"creator\"]]\n",
    "merged_all[\"creator_jobtitle\"] = [[j.get(\"job_title\") for j in json.loads(i) if isinstance(j, dict)]  if isinstance(i, str) and i != \"{}\" else pd.NA for i in merged_all[\"creator\"]]\n",
    "merged_all = merged_all.drop(columns=[\"creator\"])\n",
    "merged_all = merged_all.dropna(how=\"all\")\n",
    "merged_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2058da2-9aec-4248-bb3b-d4ed12ed6d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_all[\"creator_name\"] = [tuple(i) if isinstance(i, list) else pd.NA for i in merged_all[\"creator_name\"]]\n",
    "merged_all[\"creator_jobtitle\"] = [tuple(i) if isinstance(i, list) else pd.NA  for i in merged_all[\"creator_jobtitle\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649f41b3-b06c-494d-ab82-28b6c1b8b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist] \n",
    "\n",
    "new = []\n",
    "for num, (title, grouped) in enumerate(merged_all.groupby(\"title\")):\n",
    "    tmp = {}\n",
    "#     print(title)\n",
    "#     display(grouped)\n",
    "    tmp[\"title\"] = title\n",
    "    for col in [\"ddc_code\", \"format\", \"language\", \"publisher\", \"source\", \"type\"]:\n",
    "        tmp[col] = list(set(i for i in flatten([i for i in grouped[col]]) if not pd.isna(i)))\n",
    "    for col in [\"coverage\", \"start_semester\", \"url\", \"creator_name\", \"creator_jobtitle\", \"description\"]:\n",
    "        tmp[col] = list(set(i for i in grouped[col] if not pd.isna(i)))\n",
    "    \n",
    "    new.append(tmp)\n",
    "        \n",
    "new = pd.DataFrame(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e5139-73bd-4b9a-9ede-71af6bda5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subsumed(course):\n",
    "    rm_inds = []\n",
    "    descs = [nltk.sent_tokenize(part) for part in course]\n",
    "    for ndesc, desc in enumerate(descs):\n",
    "        for ndesc2, desc2 in enumerate(descs):\n",
    "            if ndesc == ndesc2:\n",
    "                continue\n",
    "            if all(sent in desc for sent in desc2):\n",
    "                if ndesc not in rm_inds:\n",
    "                    rm_inds.append(ndesc2)\n",
    "    return [i for n, i in enumerate(course) if n not in rm_inds]\n",
    " \n",
    "    \n",
    "def check_levenshtein(course):\n",
    "    rm_inds = []\n",
    "    for ndesc, desc in enumerate(course):\n",
    "        for ndesc2, desc2 in enumerate(course):\n",
    "            if ndesc == ndesc2:\n",
    "                continue\n",
    "            if distance(desc, desc2) / len(desc) < 0.003:\n",
    "                if ndesc not in rm_inds:\n",
    "                    rm_inds.append(ndesc2)\n",
    "    return [i for n, i in enumerate(course) if n not in rm_inds]\n",
    "\n",
    "\n",
    "def check_doubledot(course): #there are SO MANY courses where the only difference in the descriptions are stupid \"..\" instead of \".\" at sentence end\n",
    "    return [desc.strip().replace(\"...\", \"<TRIPLEDOT>\").replace(\"..\", \".\").replace(\"<TRIPLEDOT>\", \"...\") for desc in course]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceae5585-9b7d-4018-8d81-b2fe00dda9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new[\"desc_len\"] = [len([j for j in i if len(j) > 1]) for i in new[\"description\"]]\n",
    "new[\"veranstaltungsnummer\"] = pd.NA\n",
    "new[\"subtitle\"] = pd.NA\n",
    "new[\"title\"] = new[\"title\"].str.strip()\n",
    "assert len(new[new.duplicated(subset=[\"title\"])]) == 0\n",
    "assert set(type(i) for i in new[\"description\"]) == set([list])\n",
    "print(len(new))\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f404684-c110-47b9-a107-23aecf9b8e73",
   "metadata": {},
   "source": [
    "## Merging with the old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01f7832-a7c4-4bb3-838d-e7d752a93d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = pd.read_csv(\"/home/chris/Documents/UNI_neu/Masterarbeit/data_new/siddata/raw_descriptions.csv\")\n",
    "orig[\"Name\"] = orig[\"Name\"].str.strip()\n",
    "orig[\"Beschreibung\"] = orig[\"Beschreibung\"].str.strip()\n",
    "orig = orig.rename(columns=dict(VeranstaltungsNummer=\"veranstaltungsnummer\", Name=\"title\", Untertitel=\"subtitle\", Beschreibung=\"description\"))\n",
    "orig[\"description\"] = orig[\"description\"].str.strip().str.replace(\"\\r\\n\", \"\\n\")\n",
    "orig[\"description\"] = orig[\"description\"].str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\").str.replace(\"\\n\\n\", \"\\n\")\n",
    "orig[\"description\"] = orig[\"description\"].str.replace(\"\\n\", \" \")\n",
    "orig[\"description\"] = orig[\"description\"].str.strip()\n",
    "orig[\"description\"] = orig[\"description\"].str.replace(r'<.*?>', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656bb617-2bd6-4d7c-b76d-272124aafbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = orig.drop_duplicates()\n",
    "# orig_singles = orig[~orig.duplicated(subset=[\"title\"], keep=False)]\n",
    "# orig_multis = orig[orig.duplicated(subset=[\"title\"], keep=False)].drop_duplicates()\n",
    "\n",
    "new_orig = []\n",
    "for num, (title, grouped) in enumerate(orig.groupby(\"title\")):\n",
    "    tmp = {}\n",
    "#     print(title)\n",
    "#     display(grouped)\n",
    "    tmp[\"title\"] = title\n",
    "    for col in [\"veranstaltungsnummer\", \"subtitle\", \"description\"]:\n",
    "        tmp[col] = list(set(i for i in grouped[col] if not pd.isna(i)))\n",
    "    new_orig.append(tmp)\n",
    "        \n",
    "new_orig = pd.DataFrame(new_orig)\n",
    "new_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72660eea-d36f-405a-8fc9-9bdfaa4edd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = pd.concat([new, new_orig])\n",
    "alls = alls.sort_values(\"title\")\n",
    "alls_singles = alls[~alls.duplicated(subset=[\"title\"], keep=False)]\n",
    "alls_multis = alls[alls.duplicated(subset=[\"title\"], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd87c9b-790e-40f0-81b9-11bed9e738b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_alls = []\n",
    "for num, (title, grouped) in enumerate(alls_multis.groupby(\"title\")):\n",
    "    tmp = {}\n",
    "#     print(title)\n",
    "#     display(grouped)\n",
    "    tmp[\"title\"] = title\n",
    "    for col in [\"ddc_code\", \"format\", \"language\", \"publisher\", \"source\", \"type\", \"coverage\", \"start_semester\", \"url\", \"creator_name\", \"creator_jobtitle\", \"veranstaltungsnummer\", \"subtitle\", \"description\"]:\n",
    "        tmp[col] = [i for j in grouped[col] for i in (j if isinstance(j, list) else [j]) if not pd.isna(i)]\n",
    "    final_alls.append(tmp)\n",
    "        \n",
    "final_alls = pd.DataFrame(final_alls)\n",
    "final_alls = pd.concat([alls_singles, final_alls])\n",
    "final_alls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959df30f-fc81-43d1-b39b-aefbc20abeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_alls[\"n_descs\"] = [len([j for j in i if len(j) > 1]) for i in final_alls[\"description\"]]\n",
    "assert all(isinstance(i, list) for i in final_alls[\"description\"])\n",
    "print(dict(Counter(final_alls[\"n_descs\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad8615-ec71-445e-8b50-d7b5d545cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_alls[\"description\"] = [list(set(desc)) for desc in final_alls[\"description\"]]\n",
    "print(dict(Counter(final_alls[\"n_descs\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a380c3-95d2-4d3b-97c8-e34f07bbad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_alls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1e68b8-0c37-4375-b335-73925ad40513",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_alls.to_csv(\"/home/chris/Documents/UNI_neu/Masterarbeit/data_new/siddata2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c76ceb-7d1d-4af5-b4cd-811ed997fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception()\n",
    "#TODO I removed the stuff below argh -.-"
   ]
  },
  {
   "cell_type": "raw",
   "id": "591b298a-bf04-4117-b72a-c54df0fb5175",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c2bad-a49c-49a2-bd92-02191c8193e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finals = new[new[\"desc_len\"] == 1]\n",
    "doubles = new[new[\"desc_len\"] > 1].drop(columns=[\"desc_len\"])\n",
    "doubles.head()\n",
    "\n",
    "#Ok, now we need to to the smart joining of the descriptions of doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928c322-f050-49f3-8c38-405698953606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inc_doubles = {}\n",
    "# concat_descs = list(doubles[\"description\"])\n",
    "# for ncourse, course in enumerate(concat_descs):\n",
    "#     course = check_doubledot(course)\n",
    "#     course = check_subsumed(course)\n",
    "#     course = check_levenshtein(course)\n",
    "#     if len(course) == 1:\n",
    "#         inc_doubles[ncourse] = course[0]\n",
    "#     elif ncourse in SPECIALS:\n",
    "#         pass\n",
    "#     else:\n",
    "#         print(f\"{ncourse}/{len(concat_descs)}\")\n",
    "#         display(course)\n",
    "        \n",
    "#         descs = [nltk.sent_tokenize(part) for part in course]\n",
    "#         descs = list({k: None for k in flatten(descs)}.keys())\n",
    "#         merged = \" \".join(descs)\n",
    "#         print(\"merged: \", merged)\n",
    "        \n",
    "#         dowhat = input(\"[c]oncatenate? [m]erge?\")\n",
    "#         SPECIALS[ncourse] = dowhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3298dcd-b066-4633-9a1b-38b3edf40b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO now I could do some smarter stuff, like\n",
    "# * If only one word is different, just add that word\n",
    "# * Ignore special characters or just make them equal\n",
    "# ..but, whatever.\n",
    "# COULD ALSO \n",
    "# * When displaying, display in different color depending on which description it originally came from\n",
    "# * Find if somebody else had this problem already and developed something smarter than me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485883b7-d671-4b1e-82c4-370d807cb969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_course_descs(course):\n",
    "    course = check_doubledot(course)\n",
    "    course = check_subsumed(course)\n",
    "    course = check_levenshtein(course)\n",
    "    if len(course) == 1:\n",
    "        return course[0]\n",
    "    else:\n",
    "        descs = [nltk.sent_tokenize(part) for part in course]\n",
    "        descs = list({k: None for k in flatten(descs)}.keys())\n",
    "        return \" \".join(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172970d3-d538-4504-ae47-edc95af6f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_doubles = {}\n",
    "concat_descs = list(doubles[\"description\"])\n",
    "for ncourse, course in enumerate(tqdm(concat_descs)):\n",
    "    inc_doubles[ncourse] = merge_course_descs(course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ed13a-7d74-4d83-8519-e29ccae30479",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(inc_doubles) == len(doubles)\n",
    "doubles[\"description\"] = inc_doubles.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e806542-a70b-4d87-994b-78ca487a30e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finals[\"description\"] = [i[0] for i in finals[\"description\"]]\n",
    "finals = pd.concat([finals, doubles])\n",
    "finals = finals.drop(columns=[\"desc_len\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156b8617-fc58-4b48-9de9-a66951032ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "finals.to_csv(\"/home/chris/Documents/UNI_neu/Masterarbeit/data_new/tmp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7eaf6e-bd5e-407c-a504-7f9cda803935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "fbkp = deepcopy(finals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afa8d2a-dfce-4626-a011-544bc82c3643",
   "metadata": {},
   "source": [
    "# Merging with the old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586377f-f5e7-4b6b-9395-475f61ebbc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(fbkp[fbkp.duplicated(subset=[\"title\"])]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67c189-1c36-48a4-8943-0c9814d59868",
   "metadata": {},
   "outputs": [],
   "source": [
    "finals = deepcopy(fbkp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b547d-be24-4421-8d18-a4534523f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LEFT TO DO:\n",
    "# * merge with the siddata2021 dataset at least because I need the course number from there\n",
    "#    * doch die origin-column behalten, vielleicht erkenn ich da die originalen siddata2021 kurse dran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59e868-d293-4c86-b769-89921e02a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "finals[\"veranstaltungsnummer\"] = pd.NA\n",
    "finals[\"subtitle\"] = pd.NA\n",
    "finals[\"unchanged_2021\"] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d822f41b-b603-4ba0-848f-2d7b6f2b5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995845f-7761-4936-ad73-7181a57aa82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig), len(orig[\"Name\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91257e0f-29d5-412c-80db-a15b8ab8f05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "finals = deepcopy(fbkp)\n",
    "additionals = []\n",
    "for num, (name, grouped) in enumerate(tqdm(orig.groupby(\"Name\"))):\n",
    "    handled = False\n",
    "    grouped = grouped.drop_duplicates()\n",
    "    unchanged2021 = True\n",
    "    if len(grouped) > 1:\n",
    "        has_desc = [i for i in grouped[\"Beschreibung\"] if i and not pd.isna(i)]\n",
    "        if len(has_desc) > 1:\n",
    "            has_desc = merge_course_descs(has_desc)\n",
    "            unchanged2021 = False\n",
    "        merger_fn = lambda x: list({i:None for i in x.tolist() if i and not pd.isna(i)}.keys())\n",
    "        grouped = grouped.agg(dict(VeranstaltungsNummer=merger_fn, Untertitel=merger_fn)).to_frame().T\n",
    "        grouped[\"Name\"] = name\n",
    "        grouped[\"Beschreibung\"] = has_desc[0] if len(has_desc) > 0 else pd.NA\n",
    "    if len(grouped) == 1:\n",
    "        corresp = finals[finals[\"title\"] == name]\n",
    "        if len(corresp) > 0:\n",
    "            origtxt = list(grouped[\"Beschreibung\"])[0]\n",
    "            origtxt = origtxt.strip() if isinstance(origtxt, str) else \"\"\n",
    "            newtxt = list(corresp[\"description\"])[0]\n",
    "            newtxt = newtxt.strip() if isinstance(newtxt, str) else \"\"\n",
    "            row_ind = finals[finals[\"title\"] == name].index; assert len(row_ind) == 1\n",
    "            if not origtxt or origtxt == newtxt or distance(origtxt, newtxt) / len(origtxt) < 0.003:\n",
    "#                 print(\"(almost) equal!\")\n",
    "                unchanged2021 = unchanged2021 or True\n",
    "                handled = True\n",
    "            else:\n",
    "#                 print(\"merging!\")\n",
    "                unchanged2021 = False\n",
    "                merged = merge_course_descs([origtxt, newtxt])\n",
    "                finals.loc[row_ind[0], \"description\"] = merged\n",
    "                handled = True\n",
    "            if handled:\n",
    "                finals.loc[row_ind[0], \"subtitle\"] = list(grouped[\"Untertitel\"])[0]\n",
    "                finals.loc[row_ind[0], \"veranstaltungsnummer\"] = list(grouped[\"VeranstaltungsNummer\"])[0]\n",
    "                finals.loc[row_ind[0], \"unchanged_2021\"] = unchanged2021\n",
    "        else:  #do what? Add it? Add it only if it has a description? Assert that it doesn't have a description (I assume orig is a subset after all!)\n",
    "            tmp = pd.DataFrame({\"title\": grouped[\"Name\"], \"veranstaltungsnummer\": grouped[\"VeranstaltungsNummer\"], \"subtitle\": grouped[\"Untertitel\"], \"description\": grouped[\"Beschreibung\"], \"unchanged_2021\": unchanged2021})\n",
    "            additionals.append(tmp)\n",
    "            handled = True            \n",
    "    if not handled:\n",
    "        display(grouped)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028961f-eae5-4a58-baf4-7f86e52723b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(finals[finals.duplicated(subset=[\"title\"])]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6289b572-c50d-45d1-a291-8a02ef80b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "additionals = pd.concat(additionals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d125787-c7ef-425a-8223-4309a73776eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', 25, 'display.expand_frame_repr', False, 'display.max_colwidth', 400, 'display.float_format', '{:.4f}'.format):\n",
    "    display(additionals[additionals[\"unchanged_2021\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73917fb-94f0-491a-a1b0-e4946658254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 5, 'display.max_columns', 25, 'display.expand_frame_repr', False, 'display.max_colwidth', 400, 'display.float_format', '{:.4f}'.format):\n",
    "    display(finals[finals[\"unchanged_2021\"] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8adb4f0-1112-4993-9ed2-5244120ab4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "finals = pd.concat([finals, additionals])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ce763c-5445-42be-ac9a-9f741cd46af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "finals.to_csv(\"/home/chris/Documents/UNI_neu/Masterarbeit/data_new/siddata2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ea65f-b5a5-40f2-9f13-f128977d2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Rather than merging here, I can make the merging in the preprocessing - if I do that I can decide if I want to do the merging I'm doing here or just take the bag-of-words\n",
    "#      -> another way to handle duplicates is make a relative bag-of-words: if a course exists 5 times, and one word comes up in all 5 descriptions, it get's a bow-value of 1, and if it occurs only in one it has 1/5th\n",
    "#      -> I could do both if I just had a list of descriptions per name and do the processing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fcc1d-46a6-40dd-9db9-4a4f9aeab34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddd4dab-9a14-46e6-875d-dc791694139b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb3d46-d31e-4d86-908b-f3df6101b7e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa5454b-b209-43b9-a5ba-77b3af6c3854",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0be999-a63c-481e-8d00-3889ca02362c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d048b85-5132-40f1-80b1-b2d6d6fcbbea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c20ee1a-4ebd-4009-973c-e868052f423b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d72250-b3e9-465a-9540-c18f48f79b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87c194-bcaf-4c19-8f13-48b3373b1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lans = get_langs(orig[\"description\"].dropna(), assert_len=False)\n",
    "orig[\"detected_lang\"] = [lans[i] if not pd.isna(i) else None for i in orig[\"description\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8734c927-d302-40cc-865d-c8354b1ec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orig = groupby_merge(orig.drop_duplicates())\n",
    "\n",
    "filtered = filt_de(filt_len(new_orig)).drop_duplicates(subset=\"title\")\n",
    "print(\"Number of UNIQUE german courses with >= 80 words that have a veranstaltungsnummer:\", len(filtered[\"veranstaltungsnummer\"].dropna()))\n",
    "print(\"#Merged entries per title:\", dict(enumerate(new_orig[\"description\"].apply(lambda x: len(x)).value_counts()[:5])), \"...\")\n",
    "new_orig.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15413a8d-cd80-4c33-8898-fce303eb6400",
   "metadata": {},
   "source": [
    "## And Merging orig and new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f59a0f-5c45-4747-bbd8-ee4f5ea460d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orig, new = create_cols(new_orig, new)\n",
    "alls = groupby_merge(pd.concat([new, new_orig])).dropna(subset=\"description\")    #merge \n",
    "alls = alls[alls[\"description\"].apply(lambda x: not all(pd.isna(i) for i in x))] #filter those that have ANY existing description\n",
    "alls = alls.reset_index().drop_duplicates(subset=\"title\").drop(columns=\"index\")\n",
    "make_tuples(alls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc428e-541f-4e86-a740-67e762d114ad",
   "metadata": {},
   "source": [
    "#### Correct is_uos and veranstaltungsnummer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbb7e60-1d20-4c1b-90ef-b64ff0ab9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure that those that are not from the original dataset don't have a veranstaltungsnummer\n",
    "onlynotorig = lambda df: df[df[\"is_uos\"].apply(lambda x: all(pd.isna(i) for i in x))]\n",
    "assert all(pd.isna(j) for i in onlynotorig(alls)[\"veranstaltungsnummer\"].unique() for j in i)\n",
    "\n",
    "#ensure that the colum \"veranstaltungsnummer\" only exists for uos-courses\n",
    "aslist = alls[alls[\"is_uos\"].apply(lambda x: len(set(x)) > 1)][[\"is_uos\", \"veranstaltungsnummer\"]].values.tolist()\n",
    "for lislist in aslist:\n",
    "    isuos_to_num = set(list(zip(*lislist)))-{(pd.NA,pd.NA)}\n",
    "    assert all(i[0] == True for i in isuos_to_num) #ensures that if the veranstaltungsnummer is not pd.NA, is_uos must be True\n",
    "#     if len(isuos_to_num) > 1 and len(set(i[1].split(\".\")[0] for i in isuos_to_num)) > 1:\n",
    "#         #this are the (UOS!) courses that are in more than 1 FB\n",
    "#         print([i[1] for i in isuos_to_num])\n",
    "\n",
    "#now that we know that the Veranstaltungsnummer is REALLY ONLY given for the UOS-courses, we can set is_uos to True if any is_uos\n",
    "alls[\"is_uos\"] = alls[\"is_uos\"].apply(lambda x: any(i == True for i in x if not pd.isna(i)))\n",
    "alls.loc[~alls[\"is_uos\"], \"veranstaltungsnummer\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854b7ff3-f5c0-492d-8858-4e16bed5e3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of UNIQUE german courses with >= 80 words that have a veranstaltungsnummer:\", len(filt_de(filt_len(alls[~pd.isna(alls[\"veranstaltungsnummer\"])]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cedb94-7de0-400e-84ba-a339c214d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alls = squeeze_cols(alls)\n",
    "alls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc2620-2da8-4a13-9cb3-da92639e9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "alls.to_csv(\"/home/chris/Documents/UNI_neu/Masterarbeit/data_new/siddata2022_again.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3e30d-fe8a-4a99-a17f-a4bb2f7d1336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Rather than merging here, I can make the merging in the preprocessing - if I do that I can decide if I want to do the merging I'm doing here or just take the bag-of-words\n",
    "#      -> another way to handle duplicates is make a relative bag-of-words: if a course exists 5 times, and one word comes up in all 5 descriptions, it get's a bow-value of 1, and if it occurs only in one it has 1/5th\n",
    "#      -> I could do both if I just had a list of descriptions per name and do the processing later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
