"""
Run default:   `(export $(cat $MA_SELECT_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p default)`
specific file: `PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR siddata/debug_True/fautcsdp_translate_minwords100/embedding_ppmi/dissim_mat.json`
ALL Combis:    `ma_cont snakemake --cores 1 -p all --keep-going`
ALL for 1 rule `(export $(cat $MA_SELECT_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p all_for --config for_rule=create_embedding)`
from config:   `(export $(cat $MA_SELECT_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p  by_config --configfile ./config/derrac2015.yml --keep-going)`

Plot DAG:      `MA_DATASET=siddata PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -np --directory $MA_DATA_DIR --dag default | grep -A99999 "digraph" | dot -Tsvg > dag.svg`
Get results:   `rsync -az --progress $MA_DATA_DIR etlpipelines:~/data --exclude .snakemake`
"""

#TODO before writing about it: have a way of not having to set the PYTHONPATH as cmd-arg

import itertools
import os
from functools import partial
from string import Formatter
import sys

from snakemake import rules
from snakemake.io import expand, touch

from derive_conceptualspace import settings
from derive_conceptualspace.create_spaces.translate_descriptions import (
    create_languages_file as create_languages_file_base,
)
from derive_conceptualspace.create_spaces.create_embedding import (
    create_embedding as create_embedding_base,
)
from derive_conceptualspace.create_spaces.preprocess_descriptions import (
    preprocess_descriptions_full as preprocess_descriptions_base,
)
from derive_conceptualspace.create_spaces.spaces_main import (
    create_dissim_mat as create_dissim_mat_base,
)
from derive_conceptualspace.extract_keywords.keywords_main import (
    extract_candidateterms as extract_candidateterms_base,
    create_filtered_doc_cand_matrix as create_filtered_doc_cand_matrix_base,
)
from derive_conceptualspace.extract_keywords.postprocess_candidates import (
    postprocess_candidateterms as postprocess_candidateterms_base,
)
from derive_conceptualspace.pipeline import CustomContext, SnakeContext
from derive_conceptualspace.semantic_directions.create_candidate_svm import (
    create_candidate_svms as create_candidate_svms_base
)
from derive_conceptualspace.settings import (
    ENV_PREFIX,
    standardize_config,
    standardize_config_name,
    FORBIDDEN_COMBIS,
)
from misc_util.pretty_print import pretty_print as print, print_multicol
from misc_util.telegram_notifier import telegram_notify

flatten = lambda l: [item for sublist in l for item in sublist]

LAST_RULE = "create_candidate_svm"
NONDEFAULT_RULES = ["all", "default", "all_for", "by_config"]

########################################################################################################################
########################################################################################################################
######### the file `kurse-beschreibungen.csv` is the absolute basis, that one I need to get and cannot create. #########
########################################################################################################################
########################################################################################################################

def initialize_snakemake():
    if int(os.getenv(f"{ENV_PREFIX}_SNAKEMAKE_TELEGRAM", 0)):
        if not os.getenv(f"{ENV_PREFIX}_SMK_INITIALIZED"):
            print("Telegaram-Notifications ON.")
        for k, v in dict(globals()).items():
            if k.endswith("_base") and callable(v):
                globals()[k] =  telegram_notify(only_terminal=False, only_on_fail=False, log_start=True)(globals()[k])
    if any(i in sys.argv for i in NONDEFAULT_RULES):
        os.environ[f"{ENV_PREFIX}_SMK_NONDEFAULTRULE"] = [i for i in sys.argv if i in NONDEFAULT_RULES][0]
    ctx = CustomContext(SnakeContext())
    os.environ[f"{ENV_PREFIX}_SMK_INITIALIZED"] = "1"
    return ctx

def autoload_context_inputs(ctx, inputs, wildcards, params=None, input_kwargs=None, silents=None):
    #input_kwargs could eg. be: `input_kwargs={"pp_descriptions": {"metainf": {"n_samples": "ANY"}}}` to ignore the n_samples check for ONE call.
    def loader(key):
        if key in ctx.autoloader_di:
            return ctx.autoloader_di[key]
        return lambda **kwargs: kwargs[key] if key in kwargs else kwargs
    def add_to_ctx(wildcards, ctx):
        for k, v in wildcards.items():
            k, v = standardize_config(k,v)
            ctx.set_config(k,v,"smk_wildcard")
    input_kwargs = input_kwargs or {}
    add_to_ctx(wildcards, ctx)
    if params: add_to_ctx(params, ctx)
    ctx.init_context(load_envfile=True, load_conffile=False)  #after this point, new-env-vars are no considered anymore!
    for key, val in inputs.items():
        ctx.obj[key] = ctx.p.load(val, key, loader=loader(key), **input_kwargs.get(key, {}), silent=key in (silents or []))
    ctx.print_important_settings()
    # TODO overhaul: don't I need to set envvars in snakemake? -> maybe call confirm_config() here which sets all the envvars?

ctx = initialize_snakemake()

generated_paths = dict(
    pp_descriptions =          "{dataset}/debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/pp_descriptions.json",
    dissim_mat =               "{dataset}/debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/embedding_{quantification_measure}/dissim_mat.json",
    embedding =                "{dataset}/debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/embedding_{quantification_measure}/{embed_algo}_{embed_dimensions}d/embedding.json",
    candidate_terms =          "{dataset}/debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/candidate_terms_{extraction_method}.json",
    postprocessed_candidates = "{dataset}/debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/postprocessed_candidates_{extraction_method}.json",
    filtered_dcm =             "{dataset}/debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/filtered_dcm_{extraction_method}_{dcm_quant_measure}.json",
    clusters =                 "{dataset}/debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/embedding_{quantification_measure}/{embed_algo}_{embed_dimensions}d/{extraction_method}_{dcm_quant_measure}_{classifier_compareto_ranking}/clusters.json"
)
#TODO can I auto-generate these from jsonpersister?!

########################################################################################################################
########################################################################################################################
########################################################################################################################


rule get_languages:
    input:
        raw_descriptions = "{dataset}/"+ctx.get_config("raw_descriptions_file"),
    output:
        languages        = "{dataset}/"+ctx.get_config("languages_file"),
        title_languages  = "{dataset}/"+ctx.get_config("title_languages_file"),
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        create_languages_file_base(ctx.get_config("dataset")+os.sep+ctx.get_config("languages_file"), "languages", "Beschreibung", ctx.p, ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], declare_silent=True)
        create_languages_file_base(ctx.get_config("dataset")+os.sep+ctx.get_config("title_languages_file"), "title_languages", "Name", ctx.p, ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], declare_silent=True)


rule preprocess_descriptions:
    input:
        raw_descriptions   = "{dataset}/"+ctx.get_config("raw_descriptions_file"),
        languages          = "{dataset}/"+ctx.get_config("languages_file"),
        title_languages    = "{dataset}/"+ctx.get_config("title_languages_file"),
        translations       = "{dataset}/"+ctx.get_config("translations_file"),
        title_translations = "{dataset}/"+ctx.get_config("title_translations_file"),
    output:
        generated_paths["pp_descriptions"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], ctx.get_config("pp_components"), ctx.get_config("translate_policy"),
                                                             ctx.obj["languages"], ctx.obj["translations"], ctx.obj["title_languages"], ctx.obj["title_translations"])
        ctx.p.save("pp_descriptions.json", descriptions=descriptions, metainf=metainf) #TODO overhaul 16.01.2022: what about it?!


rule preprocess_descriptions_notranslate:
    input:
        raw_descriptions   = "{dataset}/"+ctx.get_config("raw_descriptions_file"),
        languages          = "{dataset}/"+ctx.get_config("languages_file"),
    output:
        generated_paths["pp_descriptions"]
    threads: 1
    run:
        assert ctx.get_config("translate_policy") != "translate", "This rule shouldn't be triggered if translate-policy == translate!!"
        autoload_context_inputs(ctx, input, wildcards)
        descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], ctx.get_config("pp_components"),
                                                             ctx.get_config("translate_policy"), ctx.obj["languages"])
        ctx.p.save("pp_descriptions.json", descriptions=descriptions, metainf=metainf)

ruleorder: preprocess_descriptions > preprocess_descriptions_notranslate


rule create_dissim_mat:
    input:
        pp_descriptions = generated_paths["pp_descriptions"]
    output:
        generated_paths["dissim_mat"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        quant_dtm, dissim_mat, metainf = create_dissim_mat_base(ctx.obj["pp_descriptions"], ctx.get_config("quantification_measure"), verbose=ctx.get_config("verbose"))
        ctx.p.save("dissim_mat.json", quant_dtm=quant_dtm, dissim_mat=dissim_mat, metainf=metainf)


rule create_embedding:
    input:
        pp_descriptions = generated_paths["pp_descriptions"], #silent
        dissim_mat = generated_paths["dissim_mat"]
    output:
        generated_paths["embedding"]
    threads: 3
    run:
        autoload_context_inputs(ctx, input, wildcards, silents=["pp_descriptions"])
        embedding = create_embedding_base(ctx.obj["dissim_mat"], ctx.get_config("embed_dimensions"), ctx.get_config("embed_algo"), verbose=ctx.get_config("verbose"))
        ctx.p.save("embedding.json", embedding=embedding)



rule extract_candidate_terms:
    input:
        pp_descriptions = generated_paths["pp_descriptions"]
    output:
        generated_paths["candidate_terms"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        candidateterms, metainf = extract_candidateterms_base(ctx.obj["pp_descriptions"], ctx.get_config("extraction_method"), ctx.get_config("max_ngram"),
                                                              ctx.get_config("faster_keybert"), verbose=ctx.get_config("verbose"))
        ctx.p.save("candidate_terms.json", candidateterms=candidateterms, metainf=metainf)


rule postprocess_candidateterms:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        candidate_terms = generated_paths["candidate_terms"],
    output:
        generated_paths["postprocessed_candidates"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        postprocessed_candidates = postprocess_candidateterms_base(ctx.obj["candidate_terms"], ctx.obj["pp_descriptions"], ctx.get_config("extraction_method"))
        ctx.p.save("postprocessed_candidates.json", postprocessed_candidates=postprocessed_candidates)


rule create_doc_cand_matrix:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        postprocessed_candidates = generated_paths["postprocessed_candidates"],
    output:
        generated_paths["filtered_dcm"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        filtered_dcm = create_filtered_doc_cand_matrix_base(ctx.obj["postprocessed_candidates"], ctx.obj["pp_descriptions"], min_term_count=ctx.get_config("candidate_min_term_count"),
                                                    dcm_quant_measure=ctx.get_config("dcm_quant_measure"), use_n_docs_count=ctx.get_config("cands_use_ndocs_count"), verbose=ctx.get_config("verbose"))
        ctx.p.save("filtered_dcm.json", doc_term_matrix=filtered_dcm, metainf={"candidate_min_term_count": ctx.get_config("candidate_min_term_count")})


rule create_candidate_svm:
    input:
        pp_descriptions = generated_paths["pp_descriptions"], #silent
        filtered_dcm = generated_paths["filtered_dcm"],
        embedding = generated_paths["embedding"]
    output:
        generated_paths["clusters"]
    threads: 3
    run:
        autoload_context_inputs(ctx, input, wildcards, silents=["pp_descriptions"])
        decision_planes, metrics = create_candidate_svms_base(ctx.obj["filtered_dcm"], ctx.obj["embedding"], ctx.obj["pp_descriptions"], verbose=ctx.get_config("verbose"))
        ctx.p.save("clusters.json", decision_planes=decision_planes, metrics=metrics) # metainf={"prim_lambda": params.prim_lambda, "sec_lambda": params.sec_lambda})

########################################################################################################################
########################################################################################################################
########################################################################################################################

def rule_by_name(name): # must be after all other rules!
    all_rules = {k: v for k, v in rules.__dict__.items() if k not in NONDEFAULT_RULES}
    return all_rules.get(name)


def expand_output(output, expand_all=False, overwriter_di=None):
    if not (any(i in sys.argv for i in NONDEFAULT_RULES) or os.getenv(f"{ENV_PREFIX}_SMK_NONDEFAULTRULE")):
        return ["Not_An_Input"] #otherwise other rules want to checkout the rules with this
    ctx.init_context(load_envfile=True, load_conffile=False)  #after this point, new env-vars are not considered
    set_val = bkp = (lambda name: settings.__dict__.get("ALL_"+standardize_config_name(name), ctx.get_config(standardize_config_name(name), silent=True))) \
                    if expand_all else (lambda name: ctx.get_config(standardize_config_name(name), silent=True))
    if overwriter_di:
        set_val = lambda name: overwriter_di[name] if name in overwriter_di else bkp(name)
    return [expand(out, **{i[1]: set_val(i[1]) for i in Formatter().parse(out) if i[1]}) for out in output]


def input_all_for(wildcards, rule_name=None):
    darule = rule_by_name(rule_name or config.get("for_rule", "preprocess_descriptions"))
    if darule:
        inputs = [i for i in flatten(expand_output(darule.output, expand_all=True)) if not any(j in i for j in FORBIDDEN_COMBIS)]
        if inputs:
            print("Running all rules for these inputs:")
            print_multicol(inputs)
        return inputs

rule all:
    input:
        partial(input_all_for, rule_name=LAST_RULE)

rule all_for:
    input:
        input_all_for

rule default:
    input:
        expand_output(rule_by_name(LAST_RULE).output)


def set_envvars_generate_input_from_config(cnf):
    """the by_config rule which allows to provide a yaml with a configuration. This reads out this configuration and
    sets the correct environment-variables for the demanded settings and lists the required output-files. It is possible
    to generate multiple outputs, so those settings that are reflected in the filenames can have lists as value, such that
    all combination of these will be tried, however those settings that must be set as env-vars cannot be lists and must
    be equal for all combinations."""
    if os.getenv(f"{ENV_PREFIX}_SMK_NONDEFAULTRULE") != "by_config":
        return ["Not_An_Input"]
    lkeys, lvals = zip(*[(k,v) for k,v in cnf.items() if isinstance(v, list)])
    othervals = {k: v for k,v in cnf.items() if k not in lkeys}
    configs_in_filename = flatten([[i[1] for i in Formatter().parse(out) if i[1]] for out in rule_by_name(LAST_RULE).output])
    assert all(i in configs_in_filename for i in lkeys), "You can only have multiple values for configs that are reflected in the filename"
    for k, v in othervals.items():
        if k not in configs_in_filename:
            os.environ[ENV_PREFIX+"_CONF_FORCE_"+standardize_config_name(k)] = str(v)
    outputs = []
    for multival_comb in itertools.product(*lvals):
        all_vals = {**dict(zip(lkeys,multival_comb)), **othervals}
        outputs.append(expand_output(rule_by_name(LAST_RULE).output, overwriter_di=all_vals))
    return outputs


rule by_config:
    input:
        set_envvars_generate_input_from_config(config)