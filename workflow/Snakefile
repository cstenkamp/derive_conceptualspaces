"""
How to plot DAG as graph:
`PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -np --directory $MA_DATA_DIR --dag | grep -A99999 "digraph" | dot -Tsvg > dag.svg`
How to run for 1 MDS:
`MA_DEBUG=1 PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR default`
or `MA_DEBUG=1 PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR tcsldp_translate/ppmi_3d/pp_keybert_count/clusters.json`
you can also: `(export $(cat $MA_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR default)`
If run on server:
`rsync -az --progress $MA_DATA_DIR etlpipelines:~/data --exclude .snakemake`
"""
import os

from snakemake.io import expand

from misc_util.pretty_print import pretty_print as print
from misc_util.telegram_notifier import telegram_notify

from derive_conceptualspace.create_spaces.translate_descriptions import (
    translate_descriptions as translate_descriptions_base,
    count_translations as count_translations_base
)
from derive_conceptualspace.extract_keywords.postprocess_candidates import (
    postprocess_candidateterms as postprocess_candidateterms_base,
)
from derive_conceptualspace.extract_keywords.keywords_main import (
    extract_candidateterms_keybert as extract_candidateterms_keybert_base,
    create_doc_cand_matrix as create_doc_cand_matrix_base,
    filter_keyphrases as filter_keyphrases_base,
)
from derive_conceptualspace.create_spaces.preprocess_descriptions import (
    preprocess_descriptions_full as preprocess_descriptions_base,
)
from derive_conceptualspace.create_spaces.spaces_main import (
    create_dissim_mat as create_dissim_mat_base,
)
from derive_conceptualspace.create_spaces.create_mds import (
    create_mds as create_mds_json_base,
)
from derive_conceptualspace.semantic_directions.create_candidate_svm import (
    create_candidate_svms as create_candidate_svms_base
)

from derive_conceptualspace.settings import (
    get_setting,
    ENV_PREFIX,
    ALL_PP_COMPONENTS,
    ALL_TRANSLATE_POLICY,
    ALL_QUANTIFICATION_MEASURE,
    ALL_DCM_QUANT_MEASURE,
    ALL_EXTRACTION_METHOD,
    ALL_MDS_DIMENSIONS,
)
from derive_conceptualspace.util.dtm_object import dtm_dissimmat_loader, dtm_loader
from derive_conceptualspace.cli.create_siddata_dataset import setup_json_persister, set_debug
from derive_conceptualspace.util.desc_object import pp_descriptions_loader

########################################################################################################################
########################################################################################################################
######### the file `kurse-beschreibungen.csv` is the absolute basis, that one I need to get and cannot create. #########
########################################################################################################################
########################################################################################################################

class Context():
    obj = {"base_dir": os.getcwd(), "add_relevantparams_to_filename": True, "verbose": False}
    auto_envvar_prefix = ENV_PREFIX


def initialize_snakemake():
    if os.getenv(f"{ENV_PREFIX}_SNAKEMAKE_TELEGRAM"):
        if not os.getenv(f"{ENV_PREFIX}_SMK_INITIALIZED"):
            print("Telegaram-Notifications ON.")
        for k, v in dict(globals()).items():
            if k.endswith("_base") and callable(v):
                globals()[k] =  telegram_notify(only_terminal=False, only_on_fail=False, log_start=True)(globals()[k])
    ctx = Context()
    json_persister = setup_json_persister(ctx, ignore_nsamples=True)
    set_debug(ctx)
    os.environ[f"{ENV_PREFIX}_SMK_INITIALIZED"] = "1"
    return ctx, json_persister

ctx, json_persister = initialize_snakemake()

def add_to_ctx(wildcards, ctx):
    for k, v in wildcards.items():
        if isinstance(v, str) and v.isnumeric():
            ctx.obj[k] = int(v)
        else:
            ctx.obj[k] = v


autoloader_di = dict(
    pp_descriptions=pp_descriptions_loader,
    dissim_mat=dtm_dissimmat_loader,
    doc_cand_matrix=dtm_loader,
    filtered_dcm=dtm_loader,
    mds=lambda **args: args["mds"],
)



def autoload_context_inputs(ctx, inputs, wildcards, params=None, input_kwargs=None):
    """
    input_kwargs could eg. be: `input_kwargs={"pp_descriptions": {"relevant_metainf": {"n_samples": "ANY"}}}` to ignore the n_samples check for ONE call.
    TO ignore it from all,
    """
    input_kwargs = input_kwargs or {}
    add_to_ctx(wildcards, ctx)
    if params: add_to_ctx(params, ctx)
    for key, val in inputs.items():
        ctx.obj[key] = json_persister.load(val, key, loader=autoloader_di.get(key), **input_kwargs.get(key, {}))


########################################################################################################################
########################################################################################################################
########################################################################################################################

n_samples = 7588 if not get_setting("DEBUG") else get_setting("DEBUG_N_ITEMS")

rule all:
    input:
        expand("{n_samples}_samples/{pp_components}_{translate_policy}/{quantification_measure}_{mds_dimensions}d/{extraction_method}_{dcm_quant_measure}/clusters.json",
                pp_components=ALL_PP_COMPONENTS, translate_policy=ALL_TRANSLATE_POLICY, quantification_measure=ALL_QUANTIFICATION_MEASURE,
                mds_dimensions=ALL_MDS_DIMENSIONS, extraction_method=ALL_EXTRACTION_METHOD, dcm_quant_measure=ALL_DCM_QUANT_MEASURE,
                n_samples=n_samples)

rule default:
    input:
        expand("{n_samples}_samples/{pp_components}_{translate_policy}/{quantification_measure}_{mds_dimensions}d/{extraction_method}_{dcm_quant_measure}/clusters.json",
                pp_components=ALL_PP_COMPONENTS[0], translate_policy=ALL_TRANSLATE_POLICY[0], quantification_measure=ALL_QUANTIFICATION_MEASURE[0],
                mds_dimensions=ALL_MDS_DIMENSIONS[0], extraction_method=ALL_EXTRACTION_METHOD[0], dcm_quant_measure=ALL_DCM_QUANT_MEASURE[0],
                n_samples=n_samples)




rule preprocess_descriptions:
    input:
        raw_descriptions = "kurse-beschreibungen.csv",
        translations = "translated_descriptions.json",
        languages = "languages.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/preprocessed_descriptions.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        vocab, descriptions = preprocess_descriptions_full_base(ctx.obj["raw_descriptions"], wildcards.pp_components, wildcards.translate_policy, ctx.obj["languages"], ctx.obj["translations"])
        json_persister.save("preprocessed_descriptions.json", vocab=vocab, descriptions=descriptions, relevant_metainf={"n_samples": len(descriptions)})


rule create_dissim_mat:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/preprocessed_descriptions.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/dissim_matrix_{quantification_measure}.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        quant_dtm, dissim_mat = create_dissim_mat_base(ctx.obj["pp_descriptions"], ctx.obj["quantification_measure"])
        json_persister.save("dissim_matrix.json", quant_dtm=quant_dtm, dissim_mat=dissim_mat)


rule create_mds:
    input:
        dissim_mat = "{n_samples}_samples/{pp_components}_{translate_policy}/dissim_matrix_{quantification_measure}.json",
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/{quantification_measure}_{mds_dimensions}d/mds.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        mds = create_mds_json_base(ctx.obj["dissim_mat"], ctx.obj["mds_dimensions"])
        json_persister.save("mds.json", mds=mds)



rule extract_candidate_terms:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/preprocessed_descriptions.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/candidate_terms_{extraction_method}.json"
    threads: 1
    params:
        faster_keybert=get_setting("FASTER_KEYBERT")
    run:
        autoload_context_inputs(ctx, input, wildcards, params)
        candidateterms = extract_candidateterms_keybert_base(ctx.obj["pp_descriptions"], ctx.obj["extraction_method"], params.faster_keybert, verbose=ctx.obj["verbose"])
        json_persister.save("candidate_terms.json", candidateterms=candidateterms, relevant_metainf={"faster_keybert": params.faster_keybert})



rule postprocess_candidateterms:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/preprocessed_descriptions.json",
        candidate_terms = "{n_samples}_samples/{pp_components}_{translate_policy}/candidate_terms_{extraction_method}.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/postprocessed_candidates_{extraction_method}.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        postprocessed_candidates = postprocess_candidateterms_base(ctx.obj["candidate_terms"], ctx.obj["pp_descriptions"], ctx.obj["extraction_method"])
        json_persister.save("postprocessed_candidates.json", postprocessed_candidates=postprocessed_candidates)


rule create_doc_cand_matrix:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/preprocessed_descriptions.json",
        postprocessed_candidates = "{n_samples}_samples/{pp_components}_{translate_policy}/postprocessed_candidates_{extraction_method}.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/doc_cand_matrix_{extraction_method}.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        doc_term_matrix = create_doc_cand_matrix_base(ctx.obj["postprocessed_candidates"], ctx.obj["pp_descriptions"], verbose=ctx.obj["verbose"])
        json_persister.save("doc_cand_matrix.json", doc_term_matrix=doc_term_matrix)


rule filter_keyphrases:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/preprocessed_descriptions.json",
        doc_cand_matrix = "{n_samples}_samples/{pp_components}_{translate_policy}/doc_cand_matrix_{extraction_method}.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/filtered_dcm_{extraction_method}_{dcm_quant_measure}.json"
    threads: 1
    params:
        candidate_min_term_count = get_setting("CANDIDATE_MIN_TERM_COUNT"),
    run:
        autoload_context_inputs(ctx, input, wildcards, params)
        filtered_dcm = filter_keyphrases_base(ctx.obj["doc_cand_matrix"], ctx.obj["pp_descriptions"], min_term_count=ctx.obj["candidate_min_term_count"], dcm_quant_measure=ctx.obj["dcm_quant_measure"], verbose=ctx.obj["verbose"])
        json_persister.save("filtered_dcm.json", relevant_metainf={"candidate_min_term_count": ctx.obj["candidate_min_term_count"]}, doc_term_matrix=filtered_dcm)



rule create_candidate_svm:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/preprocessed_descriptions.json",
        filtered_dcm = "{n_samples}_samples/{pp_components}_{translate_policy}/filtered_dcm_{extraction_method}_{dcm_quant_measure}.json",
        mds = "{n_samples}_samples/{pp_components}_{translate_policy}/{quantification_measure}_{mds_dimensions}d/mds.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/{quantification_measure}_{mds_dimensions}d/{extraction_method}_{dcm_quant_measure}/clusters.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        clusters, cluster_directions, kappa_scores, decision_planes = create_candidate_svms_base(ctx.obj["filtered_dcm"], ctx.obj["mds"], ctx.obj["pp_descriptions"], verbose=ctx.obj["verbose"])
        json_persister.save("clusters.json", clusters=clusters, cluster_directions=cluster_directions, kappa_scores=kappa_scores, decision_planes=decision_planes)