"""
    Run default:   `(export $(cat $MA_SELECT_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p default)`
or             `MA_DEBUG=1 PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR tcsldp_translate/ppmi_3d/pp_keybert_count/clusters.json`
ALL Combis:    `ma_cont snakemake --cores 1 -p  --directory /opt/data all --keep-going`
ALL for 1 rule `(export $(cat $MA_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR all_for --config for_rule=create_embedding)`
Plot DAG:      `PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -np --directory $MA_DATA_DIR --dag | grep -A99999 "digraph" | dot -Tsvg > dag.svg`
Get results:   `rsync -az --progress $MA_DATA_DIR etlpipelines:~/data --exclude .snakemake`
UPDATE: `(export $(cat $MA_SELECT_ENV_FILE | xargs) && export $(cat $MA_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR/$MA_DATASET_NAME)`
from config:   `(export $(cat $MA_SELECT_ENV_FILE | xargs) && export $(cat $MA_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR/$MA_DATASET_NAME by_config --configfile ./config/derrac2015.yml)`
"""

#TODO before writing about it: have a way of not having to set the PYTHONPATH as cmd-arg

import os
from functools import partial

from snakemake import rules
from snakemake.io import expand, touch

from misc_util.pretty_print import pretty_print as print
from misc_util.telegram_notifier import telegram_notify

from derive_conceptualspace.create_spaces.translate_descriptions import (
    full_translate_titles as translate_titles_base,
    full_translate_descriptions as translate_descriptions_base,
    # count_translations as count_translations_base
)
from derive_conceptualspace.extract_keywords.postprocess_candidates import (
    postprocess_candidateterms as postprocess_candidateterms_base,
)
from derive_conceptualspace.extract_keywords.keywords_main import (
    extract_candidateterms as extract_candidateterms_base,
    create_filtered_doc_cand_matrix as create_filtered_doc_cand_matrix_base,
)
from derive_conceptualspace.create_spaces.preprocess_descriptions import (
    preprocess_descriptions_full as preprocess_descriptions_base,
)
from derive_conceptualspace.create_spaces.spaces_main import (
    create_dissim_mat as create_dissim_mat_base,
)
from derive_conceptualspace.create_spaces.create_embedding import (
    create_embedding as create_embedding_base,
)
from derive_conceptualspace.semantic_directions.create_candidate_svm import (
    create_candidate_svms as create_candidate_svms_base
)
from derive_conceptualspace.pipeline import CustomContext, SnakeContext

from derive_conceptualspace.settings import (
    get_envvarname,
    get_setting,
    ENV_PREFIX,
    set_envvar,
    get_envvar,
    FORBIDDEN_COMBIS, standardize_config, standardize_config_name
)
import itertools
import os
from string import Formatter

from snakemake.io import expand

from derive_conceptualspace.settings import get_setting, FORBIDDEN_COMBIS, get_envvarname
from misc_util.pretty_print import print_multicol
from derive_conceptualspace import settings

flatten = lambda l: [item for sublist in l for item in sublist]

from derive_conceptualspace.pipeline import setup_json_persister, get_jsonpersister_args

LAST_RULE = "create_candidate_svm"

########################################################################################################################
########################################################################################################################
######### the file `kurse-beschreibungen.csv` is the absolute basis, that one I need to get and cannot create. #########
########################################################################################################################
########################################################################################################################

# overwritable = lambda param, value: get_envvar(envvarname = ENV_PREFIX + "_" + param.upper().replace("-","_")) or value


def initialize_snakemake():
    if int(os.getenv(f"{ENV_PREFIX}_SNAKEMAKE_TELEGRAM", 0)):
        if not os.getenv(f"{ENV_PREFIX}_SMK_INITIALIZED"):
            print("Telegaram-Notifications ON.")
        for k, v in dict(globals()).items():
            if k.endswith("_base") and callable(v):
                globals()[k] =  telegram_notify(only_terminal=False, only_on_fail=False, log_start=True)(globals()[k])
    ctx = CustomContext(SnakeContext(cwd=os.getcwd()))
    ctx.init_context(load_envfile=True)  #after this point, no new env-vars should be set anymore (are not considered)
    #we don't load the conffile because in snakemake that has a different meaning #TODO overhaul really?!
    #TODO overhaul 16.01.2022: is the context consistent with all ways of doing things here (by filename, by config)???
    os.environ[f"{ENV_PREFIX}_SMK_INITIALIZED"] = "1"
    return ctx

ctx = initialize_snakemake()

def add_to_ctx(wildcards, ctx):
    for k, v in wildcards.items():
        k, v = standardize_config(k, v)
        ctx.set_config(k, v, "smk_wildcard")
        # set_envvar(ENV_PREFIX+"_"+k.upper().replace("-","_"), v) #TODO use the one function for this
        # TODO overhaul 16.01.2022: I need to set envvars in snakemake


def autoload_context_inputs(ctx, inputs, wildcards, params=None, input_kwargs=None):
    """
    input_kwargs could eg. be: `input_kwargs={"pp_descriptions": {"relevant_metainf": {"n_samples": "ANY"}}}` to ignore the n_samples check for ONE call.
    TO ignore it from all, #TODO overhaul 16.01.2022: hä?!
    """
    def loader(key):
        if key in ctx.autoloader_di:
            return ctx.autoloader_di[key]
        return lambda **kwargs: kwargs[key] if key in kwargs else kwargs
    input_kwargs = input_kwargs or {}
    add_to_ctx(wildcards, ctx)
    if params: add_to_ctx(params, ctx)
    for key, val in inputs.items():
        ctx.obj[key] = ctx.p.load(val, key, loader=loader(key), **input_kwargs.get(key, {}))
    #TODO overhaul 16.01.2022: hier könnte man jetzt tatsächlich confirm_config aufrufen und das setzt dann die ganzen envvars

########################################################################################################################
########################################################################################################################
########################################################################################################################

rule trigger_default:
    input:
        ".alldone"

########################################################################################################################


generated_paths = dict(
    pp_descriptions = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/pp_descriptions.json",
    dissim_mat = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/embedding_{quantification_measure}/dissim_mat.json",
    embedding = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/embedding_{quantification_measure}/{embed_algo}_{embed_dimensions}d/embedding.json",
    candidate_terms = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/candidate_terms_{extraction_method}.json",
    postprocessed_candidates = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/postprocessed_candidates_{extraction_method}.json",
    filtered_dcm = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/filtered_dcm_{extraction_method}_{dcm_quant_measure}.json",
    clusters = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/embedding_{quantification_measure}/{embed_algo}_{embed_dimensions}d/{extraction_method}_{dcm_quant_measure}/clusters.json"
)


rule preprocess_descriptions:
    input:
        raw_descriptions = "kurse-beschreibungen.csv",
        translations = "translated_descriptions.json",
        languages = "languages.json",
        title_languages = "title_languages.json",
        title_translations = "translated_titles.json",
    output:
        generated_paths["pp_descriptions"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], ctx.get_config("pp_components"), ctx.get_config("translate_policy"),
                                                             ctx.obj["languages"], ctx.obj["translations"], ctx.obj["title_languages"], ctx.obj["title_translations"])
        ctx.p.save("pp_descriptions.json", descriptions=descriptions)#, relevant_metainf=metainf) #TODO overhaul 16.01.2022: what about it?!

#TODO overhaul 16.01.2022: add back
# rule preprocess_descriptions_notranslate:
#     input:
#         raw_descriptions = overwritable("raw_descriptions_file", "kurse-beschreibungen.csv"), #TODO add this to the schema of the rest
#         languages = "languages.json",
#     output:
#         generated_paths["pp_descriptions"]
#     threads: 1
#     run:
#         assert wildcards.translate_policy != "translate"
#         autoload_context_inputs(ctx, input, wildcards)
#         descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], wildcards.pp_components, wildcards.translate_policy, ctx.obj["languages"])
#         ctx.p.save("pp_descriptions.json", descriptions=descriptions, relevant_metainf=metainf)
#
# ruleorder: preprocess_descriptions > preprocess_descriptions_notranslate


rule create_dissim_mat:
    input:
        pp_descriptions = generated_paths["pp_descriptions"]
    output:
        generated_paths["dissim_mat"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        quant_dtm, dissim_mat, metainf = create_dissim_mat_base(ctx.obj["pp_descriptions"], ctx.get_config("quantification_measure"), ctx.get_config("verbose"))
        ctx.p.save("dissim_mat.json", quant_dtm=quant_dtm, dissim_mat=dissim_mat)#, relevant_metainf=metainf)


rule create_embedding:
    input:
        dissim_mat = generated_paths["dissim_mat"]
    output:
        generated_paths["embedding"]
    threads: 3
    run:
        autoload_context_inputs(ctx, input, wildcards)
        embedding = create_embedding_base(ctx.obj["dissim_mat"], ctx.get_config("embed_dimensions"), ctx.get_config("embed_algo"))
        ctx.p.save("embedding.json", embedding=embedding)



rule extract_candidate_terms:
    input:
        pp_descriptions = generated_paths["pp_descriptions"]
    output:
        generated_paths["candidate_terms"]
    threads: 1
    params:
        faster_keybert=get_setting("FASTER_KEYBERT"), #TODO overhaul 16.01.2022: this is not how it's supposed to be anymore, is it?!
        max_ngram=get_setting("MAX_NGRAM")
    run:
        autoload_context_inputs(ctx, input, wildcards, params)
        candidateterms, relevant_metainf = extract_candidateterms_base(ctx.obj["pp_descriptions"], ctx.get_config("extraction_method"), ctx.get_config("max_ngram"),
                                                                       ctx.get_config("faster_keybert"), verbose=ctx.get_config("verbose"))
        ctx.p.save("candidate_terms.json", candidateterms=candidateterms)#, relevant_metainf=relevant_metainf)


rule postprocess_candidateterms:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        candidate_terms = generated_paths["candidate_terms"],
    output:
        generated_paths["postprocessed_candidates"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        postprocessed_candidates = postprocess_candidateterms_base(ctx.obj["candidate_terms"], ctx.obj["pp_descriptions"], ctx.get_config("extraction_method"))
        ctx.p.save("postprocessed_candidates.json", postprocessed_candidates=postprocessed_candidates)


rule create_doc_cand_matrix:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        postprocessed_candidates = generated_paths["postprocessed_candidates"],
    output:
        generated_paths["filtered_dcm"]
    threads: 1
    params:
        candidate_min_term_count = get_setting("CANDIDATE_MIN_TERM_COUNT"),
        use_ndocs_count = get_setting("CANDS_USE_NDOCS_COUNT")
    run:
        autoload_context_inputs(ctx, input, wildcards, params)
        filtered_dcm = create_filtered_doc_cand_matrix_base(ctx.obj["postprocessed_candidates"], ctx.obj["pp_descriptions"], min_term_count=ctx.get_config("candidate_min_term_count"),
                                                    dcm_quant_measure=ctx.get_config("dcm_quant_measure"), use_n_docs_count=ctx.get_config("use_ndocs_count"), verbose=ctx.get_config("verbose"))
        ctx.p.save("filtered_dcm.json", doc_term_matrix=filtered_dcm) #relevant_metainf={"candidate_min_term_count": ctx.obj["candidate_min_term_count"]}


rule create_candidate_svm:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        filtered_dcm = generated_paths["filtered_dcm"],
        embedding = generated_paths["embedding"]
    output:
        generated_paths["clusters"]
    threads: 3
    # params:
    #     prim_lambda = get_setting("PRIM_LAMBDA"),
    #     sec_lambda = get_setting("SEC_LAMBDA")
    run:
        autoload_context_inputs(ctx, input, wildcards)
        decision_planes, metrics = create_candidate_svms_base(ctx.obj["filtered_dcm"], ctx.obj["embedding"], ctx.obj["pp_descriptions"], verbose=ctx.get_config("verbose"))
        ctx.p.save("clusters.json", decision_planes=decision_planes, metrics=metrics)
                            # relevant_metainf={"prim_lambda": params.prim_lambda, "sec_lambda": params.sec_lambda})

########################################################################################################################
########################################################################################################################
########################################################################################################################

def rule_by_name(name): # must be after all other rules!
    all_rules = {k: v for k, v in rules.__dict__.items() if k not in ["all", "default", "all_for", "trigger_default"]}
    return all_rules.get(name)


def expand_output(output, expand_all=False, overwriter_di=None):
    set_val = bkp = (lambda name: settings.__dict__.get("ALL_"+standardize_config_name(name), ctx.get_config(standardize_config_name(name), silent=True))) \
                    if expand_all else (lambda name: ctx.get_config(standardize_config_name(name), silent=True))
    if overwriter_di:
        set_val = lambda name: overwriter_di[name] if name in overwriter_di else bkp(name)
    return [expand(out, **{i[1]: set_val(i[1]) for i in Formatter().parse(out) if i[1]}) for out in output]


# def input_all_for(wildcards, rule_name=None):
#     darule = rule_by_name(rule_name or config.get("for_rule", "preprocess_descriptions"))
#     if darule:
#         inputs = [i for i in flatten(expand_output(darule.output, expand_all=True)) if not any(j in i for j in FORBIDDEN_COMBIS)]
#         if inputs:
#             print("Running all rules for these inputs:")
#             print_multicol(inputs)
#         return inputs

# def set_envvars_generate_input_from_config(cnf):
#     """the by_config rule which allows to provide a yaml with a configuration. This reads out this configuration and
#     sets the correct environment-variables for the demanded settings and lists the required output-files. It is possible
#     to generate multiple outputs, so those settings that are reflected in the filenames can have lists as value, such that
#     all combination of these will be tried, however those settings that must be set as env-vars cannot be lists and must
#     be equal for all combinations."""
#     lkeys, lvals = zip(*[(k,v) for k,v in cnf.items() if isinstance(v, list)])
#     othervals = {k: v for k,v in cnf.items() if k not in lkeys}
#     configs_in_filename = flatten([[i[1] for i in Formatter().parse(out) if i[1]] for out in rule_by_name(LAST_RULE).output])
#     assert all(i in configs_in_filename for i in lkeys), "You can only have multiple values for configs that are reflected in the filename"
#     for k, v in othervals.items():
#         if k not in configs_in_filename:
#             os.environ.setdefault(get_envvarname(k), str(v))
#             os.environ[get_envvarname(k)] = str(v)
#     outputs = []
#     for multival_comb in itertools.product(*lvals):
#         all_vals = {**dict(zip(lkeys,multival_comb)), **othervals}
#         outputs.append(expand_output(rule_by_name(LAST_RULE).output, overwriter_di=all_vals))
#     return outputs


# rule all:
#     input:
#         partial(input_all_for, rule_name=LAST_RULE)
#
# rule all_for:
#     input:
#         input_all_for

rule default:
    input:
        expand_output(rule_by_name(LAST_RULE).output)
    output:
        touch(".alldone")

# rule by_config:
#     input:
#         set_envvars_generate_input_from_config(config)
