"""
Plot DAG:      `PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -np --directory $MA_DATA_DIR --dag | grep -A99999 "digraph" | dot -Tsvg > dag.svg`
Run default:   `MA_DEBUG=1 PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR default`
or             `MA_DEBUG=1 PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR tcsldp_translate/ppmi_3d/pp_keybert_count/clusters.json`
or:            `(export $(cat $MA_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR default)`
ALL Combis:    `ma_cont snakemake --cores 1 -p  --directory /opt/data all --keep-going`
Get results:   `rsync -az --progress $MA_DATA_DIR etlpipelines:~/data --exclude .snakemake`
"""

#TODO before writing about it: have a way of not having to set the PYTHONPATH as cmd-arg

import os

from snakemake.io import expand

from misc_util.pretty_print import pretty_print as print
from misc_util.telegram_notifier import telegram_notify

from derive_conceptualspace.create_spaces.translate_descriptions import (
    translate_descriptions as translate_descriptions_base,
    count_translations as count_translations_base
)
from derive_conceptualspace.extract_keywords.postprocess_candidates import (
    postprocess_candidateterms as postprocess_candidateterms_base,
)
from derive_conceptualspace.extract_keywords.keywords_main import (
    extract_candidateterms_keybert as extract_candidateterms_keybert_base,
    create_doc_cand_matrix as create_doc_cand_matrix_base,
    filter_keyphrases as filter_keyphrases_base,
)
from derive_conceptualspace.create_spaces.preprocess_descriptions import (
    preprocess_descriptions_full as preprocess_descriptions_base,
)
from derive_conceptualspace.create_spaces.spaces_main import (
    create_dissim_mat as create_dissim_mat_base,
)
from derive_conceptualspace.create_spaces.create_embedding import (
    create_embedding as create_embedding_base,
)
from derive_conceptualspace.semantic_directions.create_candidate_svm import (
    create_candidate_svms as create_candidate_svms_base
)

from derive_conceptualspace.settings import (
    get_setting,
    ENV_PREFIX,
    ALL_PP_COMPONENTS,
    ALL_TRANSLATE_POLICY,
    ALL_QUANTIFICATION_MEASURE,
    ALL_DCM_QUANT_MEASURE,
    ALL_EXTRACTION_METHOD,
    ALL_EMBED_DIMENSIONS,
    ALL_EMBED_ALGO,
    FORBIDDEN_COMBIS, set_envvar
)
from derive_conceptualspace.util.dtm_object import dtm_dissimmat_loader, dtm_loader
from derive_conceptualspace.cli.create_siddata_dataset import setup_json_persister, set_debug, get_jsonpersister_args
from derive_conceptualspace.util.desc_object import pp_descriptions_loader
from misc_util.logutils import CustomIO

########################################################################################################################
########################################################################################################################
######### the file `kurse-beschreibungen.csv` is the absolute basis, that one I need to get and cannot create. #########
########################################################################################################################
########################################################################################################################

class Context():
    obj = {"base_dir": os.getcwd(), "add_relevantparams_to_filename": True, "verbose": False}
    auto_envvar_prefix = ENV_PREFIX


def initialize_snakemake():
    if int(os.getenv(f"{ENV_PREFIX}_SNAKEMAKE_TELEGRAM", 0)):
        if not os.getenv(f"{ENV_PREFIX}_SMK_INITIALIZED"):
            print("Telegaram-Notifications ON.")
        for k, v in dict(globals()).items():
            if k.endswith("_base") and callable(v):
                globals()[k] =  telegram_notify(only_terminal=False, only_on_fail=False, log_start=True)(globals()[k])
    ctx = Context()
    CustomIO.init()
    json_persister = setup_json_persister(ctx, ignore_nsamples=True)
    set_debug(ctx)
    os.environ[f"{ENV_PREFIX}_SMK_INITIALIZED"] = "1"
    return ctx, json_persister

ctx, json_persister = initialize_snakemake()

def add_to_ctx(wildcards, ctx):
    for k, v in wildcards.items():
        if isinstance(v, str) and v.isnumeric():
            ctx.obj[k] = int(v)
        else:
            ctx.obj[k] = v
        set_envvar(ENV_PREFIX+"_"+k.upper().replace("-","_"), ctx.obj[k])

autoloader_di = dict(
    pp_descriptions=pp_descriptions_loader,
    dissim_mat=dtm_dissimmat_loader,
    doc_cand_matrix=dtm_loader,
    filtered_dcm=dtm_loader,
    embedding=lambda **args: args["embedding"],
)

def autoload_context_inputs(ctx, inputs, wildcards, params=None, input_kwargs=None):
    """
    input_kwargs could eg. be: `input_kwargs={"pp_descriptions": {"relevant_metainf": {"n_samples": "ANY"}}}` to ignore the n_samples check for ONE call.
    TO ignore it from all,
    """
    input_kwargs = input_kwargs or {}
    add_to_ctx(wildcards, ctx)
    if params: add_to_ctx(params, ctx)
    for key, val in inputs.items():
        ctx.obj[key] = json_persister.load(val, key, loader=autoloader_di.get(key), **input_kwargs.get(key, {}))


########################################################################################################################
########################################################################################################################
########################################################################################################################

n_samples = 7588 if not get_setting("DEBUG") else get_setting("DEBUG_N_ITEMS")
dir_struct = get_jsonpersister_args()[2]


rule all:
    input:
        [i for i in
            expand(os.sep.join(dir_struct+["clusters.json"]),
                    pp_components=ALL_PP_COMPONENTS, translate_policy=ALL_TRANSLATE_POLICY, quantification_measure=ALL_QUANTIFICATION_MEASURE,
                    embed_dimensions=ALL_EMBED_DIMENSIONS, extraction_method=ALL_EXTRACTION_METHOD, dcm_quant_measure=ALL_DCM_QUANT_MEASURE,
                    embed_algo=ALL_EMBED_ALGO, n_samples=n_samples)
            if not any(j in i for j in FORBIDDEN_COMBIS)
        ]

rule default:
    input:
        expand(os.sep.join(dir_struct+["clusters.json"]),
                pp_components=get_setting("PP_COMPONENTS"), translate_policy=get_setting("TRANSLATE_POLICY"), quantification_measure=get_setting("QUANTIFICATION_MEASURE"),
                embed_dimensions=get_setting("EMBED_DIMENSIONS"), extraction_method=get_setting("EXTRACTION_METHOD"), dcm_quant_measure=get_setting("DCM_QUANT_MEASURE"),
                embed_algo=get_setting("EMBED_ALGO"), n_samples=n_samples)


rule preprocess_descriptions:
    input:
        raw_descriptions = "kurse-beschreibungen.csv",
        translations = "translated_descriptions.json",
        languages = "languages.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/pp_descriptions.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        vocab, descriptions = preprocess_descriptions_base(ctx.obj["raw_descriptions"], wildcards.pp_components, wildcards.translate_policy, ctx.obj["languages"], ctx.obj["translations"])
        json_persister.save("pp_descriptions.json", vocab=vocab, descriptions=descriptions, relevant_metainf={"n_samples": len(descriptions)})


rule create_dissim_mat:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/pp_descriptions.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/dissim_mat_{quantification_measure}.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        quant_dtm, dissim_mat = create_dissim_mat_base(ctx.obj["pp_descriptions"], ctx.obj["quantification_measure"])
        json_persister.save("dissim_mat.json", quant_dtm=quant_dtm, dissim_mat=dissim_mat)


rule create_embedding:
    input:
        dissim_mat = "{n_samples}_samples/{pp_components}_{translate_policy}/dissim_mat_{quantification_measure}.json",
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/{quantification_measure}_{embed_algo}_{embed_dimensions}d/embedding.json"
    threads: 3
    run:
        autoload_context_inputs(ctx, input, wildcards)
        embedding = create_embedding_base(ctx.obj["dissim_mat"], ctx.obj["embed_dimensions"], ctx.obj["embed_algo"])
        json_persister.save("embedding.json", embedding=embedding)



rule extract_candidate_terms:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/pp_descriptions.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/candidate_terms_{extraction_method}.json"
    threads: 1
    params:
        faster_keybert=get_setting("FASTER_KEYBERT")
    run:
        autoload_context_inputs(ctx, input, wildcards, params)
        candidateterms = extract_candidateterms_keybert_base(ctx.obj["pp_descriptions"], ctx.obj["extraction_method"], params.faster_keybert, verbose=ctx.obj["verbose"])
        json_persister.save("candidate_terms.json", candidateterms=candidateterms, relevant_metainf={"faster_keybert": params.faster_keybert})



rule postprocess_candidateterms:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/pp_descriptions.json",
        candidate_terms = "{n_samples}_samples/{pp_components}_{translate_policy}/candidate_terms_{extraction_method}.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/postprocessed_candidates_{extraction_method}.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        postprocessed_candidates = postprocess_candidateterms_base(ctx.obj["candidate_terms"], ctx.obj["pp_descriptions"], ctx.obj["extraction_method"])
        json_persister.save("postprocessed_candidates.json", postprocessed_candidates=postprocessed_candidates)


rule create_doc_cand_matrix:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/pp_descriptions.json",
        postprocessed_candidates = "{n_samples}_samples/{pp_components}_{translate_policy}/postprocessed_candidates_{extraction_method}.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/doc_cand_matrix_{extraction_method}.json"
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        doc_term_matrix = create_doc_cand_matrix_base(ctx.obj["postprocessed_candidates"], ctx.obj["pp_descriptions"], verbose=ctx.obj["verbose"])
        json_persister.save("doc_cand_matrix.json", doc_term_matrix=doc_term_matrix)


rule filter_keyphrases:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/pp_descriptions.json",
        doc_cand_matrix = "{n_samples}_samples/{pp_components}_{translate_policy}/doc_cand_matrix_{extraction_method}.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/filtered_dcm_{extraction_method}_{dcm_quant_measure}.json"
    threads: 1
    params:
        candidate_min_term_count = get_setting("CANDIDATE_MIN_TERM_COUNT"),
    run:
        autoload_context_inputs(ctx, input, wildcards, params)
        filtered_dcm = filter_keyphrases_base(ctx.obj["doc_cand_matrix"], ctx.obj["pp_descriptions"], min_term_count=ctx.obj["candidate_min_term_count"], dcm_quant_measure=ctx.obj["dcm_quant_measure"], verbose=ctx.obj["verbose"])
        json_persister.save("filtered_dcm.json", relevant_metainf={"candidate_min_term_count": ctx.obj["candidate_min_term_count"]}, doc_term_matrix=filtered_dcm)



rule create_candidate_svm:
    input:
        pp_descriptions = "{n_samples}_samples/{pp_components}_{translate_policy}/pp_descriptions.json",
        filtered_dcm = "{n_samples}_samples/{pp_components}_{translate_policy}/filtered_dcm_{extraction_method}_{dcm_quant_measure}.json",
        embedding = "{n_samples}_samples/{pp_components}_{translate_policy}/{quantification_measure}_{embed_algo}_{embed_dimensions}d/embedding.json"
    output:
        "{n_samples}_samples/{pp_components}_{translate_policy}/{quantification_measure}_{embed_algo}_{embed_dimensions}d/{extraction_method}_{dcm_quant_measure}/clusters.json"
    threads: 3
    params:
        prim_lambda = get_setting("PRIM_LAMBDA"),
        sec_lambda = get_setting("SEC_LAMBDA")
    run:
        autoload_context_inputs(ctx, input, wildcards, params=params)
        clusters, cluster_directions, kappa_scores, decision_planes = create_candidate_svms_base(ctx.obj["filtered_dcm"], ctx.obj["embedding"], ctx.obj["pp_descriptions"],
                                                                        prim_lambda=ctx.obj["prim_lambda"], sec_lambda=ctx.obj["sec_lambda"], verbose=ctx.obj["verbose"])
        json_persister.save("clusters.json", clusters=clusters, cluster_directions=cluster_directions, kappa_scores=kappa_scores, decision_planes=decision_planes,
                            relevant_metainf={"prim_lambda": params.prim_lambda, "sec_lambda": params.sec_lambda})