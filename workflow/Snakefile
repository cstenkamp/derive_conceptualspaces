from pathlib import Path
import os
from os.path import join as pjoin

from derive_conceptualspace.extract_keywords.keywords_main import (
    extract_candidateterms_keybert_preprocessed as extract_candidateterms_keybert_preprocessed_base,
)
from derive_conceptualspace.util.jsonloadstore import Struct, json_dump
from derive_conceptualspace.settings import ORIGLAN, ONLYENG, TRANSL
from derive_conceptualspace.create_spaces.spaces_main import (
    create_dissim_mat as create_dissim_mat_base,
    create_mds_json as create_mds_json_base,
    preprocess_descriptions_full as preprocess_descriptions_full_base, load_preprocessed_descriptions,
)
from misc_util.telegram_notifier import telegram_notify

if os.getenv("MA_SMK_TGRAM"):
    print("Telegaram-Notifications ON.")
    for k, v in dict(globals()).items():
        if k.endswith("_base") and callable(v):
            globals()[k] =  telegram_notify(only_terminal=False, only_on_fail=False, log_start=True)(globals()[k])

def cnf(name, val):
    globals()[name] = config.setdefault(name, val)

QUANTIFICATION_MEASURES = ["ppmi", "tf-idf"]
MDS_DIMENSIONS = [3, 50, 100]

########################################################################################
#the file `kurse-beschreibungen.csv` is the absolute basis, that one I need to get and cannot create.
########################################################################################

#how-to plot graph (KEEP ME): `PYTHONPATH=. snakemake --cores 1 -np --directory /home/chris/Documents/UNI_neu/Masterarbeit/DATA_CLONE_3/  --dag | grep -A99999 "digraph" | dot -Tsvg > dag.svg`
#run as eg. `MA_DEBUG=1 PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory /home/chris/Documents/UNI_neu/Masterarbeit/DATA_CLONE_3/`

#run as eg.  PYTHONPATH=/home/chris/Documents/UNI_neu/Masterarbeit/Derive_Conceptualspace snakemake --cores 1 -p "siddata_names_descriptions_mds_3.json" --directory /home/chris/Documents/UNI_neu/Masterarbeit/DATA_CLONE/
#  crsync /home/chris/Documents/UNI_neu/Masterarbeit/DATA_CLONE/ etlpipelines:~/data --exclude .snakemake

# cnf("DATA_BASE", ".")

# from shutil import which
# # Intermediate dirs
# cnf("VIDEO_BASE", ".")
# cnf("GCN_CONFIG", pjoin(WORK, "gcn_config.yaml"))
# cnf("FFPROBE_BIN", which("ffprobe"))
# include: "rules/skels.smk"

# rule setup:
#     "Perform setup actions"
#     input:
#         GCN_CONFIG
# def all(ext):
#     base, = glob_wildcards(pjoin(VIDEO_BASE, "{base}.mp4"))
#     return [fn + ext for fn in base]


rule all:
    input:
        expand("mds_{mds_dims}d_{quant_meas}.json", quant_meas=QUANTIFICATION_MEASURES, mds_dims=MDS_DIMENSIONS)


rule preprocess_descriptions:
    input:
        "kurse-beschreibungen.csv",
        "translated_descriptions.json"
    output:
        "preprocessed_descriptions.json"
    params:
        translate_policy=TRANSL,
        pp_components=dict(
            sent_tokenize=True,
            convert_lower=True,
            remove_stopwords=True,
            lemmatize=True,
            remove_diacritics=True,
            remove_punctuation=True,
        )
    run:
        vocab, descriptions = preprocess_descriptions_full_base(os.getcwd(), params.translate_policy, params.pp_components)
        descriptions = [Struct(**desc.__dict__) for desc in descriptions]
        json_dump({"vocab": vocab, "descriptions": descriptions, "pp_components": params.pp_components}, output)


rule create_dissim_mat:
    input:
        descs="preprocessed_descriptions.json"
    output:
        "dissim_matrix_{quantification_measure}.json"
    run:
        quantification, dissim_mat, pp_components = create_dissim_mat_base(os.getcwd(), input.descs, wildcards.quantification_measure)
        quantification = Struct(**{k:v for k,v in quantification.__dict__.items() if not k.startswith("_") and k not in ["csr_matrix", "doc_freqs", "reverse_term_dict"]})
        json_dump({"quantification": quantification, "dissim_mat": dissim_mat, "pp_components": pp_components, "quant_measure": wildcards.quantification_measure}, output)


rule create_mds:
    input:
        dissim_mat="dissim_matrix_{quantification_measure}.json"
    output:
        "mds_{mds_dims}d_{quantification_measure}.json"
    run:
        mds = create_mds_json_base(os.getcwd(), input.dissim_mat, int(wildcards.mds_dims))
        mds["mds"] = Struct(**mds["mds"].__dict__)
        json_dump(mds, input.dissim_mat.replace("dissim_matrix", "mds_"+wildcards.mds_dims+"d"))


rule create_candidate_terms_pp:
    input:
        descs = "preprocessed_descriptions.json"
    output:
        "candidate_terms.json"
    run:
        vocab, descriptions, pp_components = load_preprocessed_descriptions(input.descs)
        candidates, model_name = extract_candidateterms_keybert_preprocessed_base(vocab, descriptions, faster_keybert=False)
        res = {"model": model_name, "candidate_terms":candidates, "pp_txt_for_cands": True, "pp_components": pp_components, "pp_descriptions_filename": input.descs}
        json_dump(res, "candidate_terms.json")


#
# checkpoint create_mds:
#     "Produce the Siddata Dataset from the kurse-beschreibungen CSV"
#     input:
#         "kurse-beschreibungen.csv"
#     output:
#         "siddata_names_descriptions_mds_{n_dims}.json"
#     run:
#         print(f"running pipeline for {wildcards.n_dims} dims")
#         create_mds(f"siddata_names_descriptions_mds_{wildcards.n_dims}.json", wildcards.n_dims, from_csv_path=".", to_data_path=".")
#
#
# def input_for_translate_descriptions(wildcards):
#     try:
#         return next(Path(".").glob("siddata_names_descriptions_mds_*.json"))
#     except StopIteration as err:
#         raise Exception("No n_dims JSON files found") from err
#
# rule translate_descriptions:
#     input:
#         input_for_translate_descriptions
#     output:
#         "translated_descriptions.json",
#     shell:
#         "echo {input} &&"
#         "touch {output}"
#
#
# rule create_desc15style_mds:
#     input:
#         "translated_descriptions.json", #TODO ists ja nur bei translate_policy=TRANSL
#         "siddata_names_descriptions_mds_{n_dims}.json"
#     output:
#         # "languages.json",  #cannot have multiple outputs, only one of which with wildcards :/
#         "d{n_dims}/courses{n_dims}.mds",
#         # "courseNames.txt" #TODO "course" noch als variable?
#     run:
#         create_descstyle_dataset(wildcards.n_dims, "courses", from_path=".", to_path=".", translate_policy=ORIGLAN)
#
#
# rule extract_candidateterms_keybert:
#     input:
#         input_for_translate_descriptions
#     output:
#         "candidate_terms.json"
#     run:
#         extract_candidateterms_keybert.callback(".")
#
#
# rule postprocess_candidateterms:
#     input:
#         input_for_translate_descriptions,
#         "candidate_terms.json"
#     output:
#         "candidate_terms_postprocessed.json"
#     run:
#         postprocess_candidateterms.callback(".", postfix="_postprocessed")
#
#
# rule create_doc_term_matrix:
#     input:
#         input_for_translate_descriptions,
#         "candidate_terms_postprocessed.json"
#     output:
#         "doc_term_matrix.json"
#     run:
#         create_doc_term_matrix.callback(".")
#
#
#
#


# rule get_gcn_weights:
#     output:
#         directory(GCN_WEIGHTS)
#     shell:
#         "mkdir -p " + GCN_WEIGHTS + " && " +
#         "cd " + GCN_WEIGHTS + " && " +
#         "wget http://guanghan.info/download/Data/LightTrack/weights/GCN.zip && " +
#         "unzip GCN.zip"
#
# rule tmpl_gcn_config:
#     input:
#         GCN_WEIGHTS
#     output:
#         GCN_CONFIG
#     run:
#         open(GCN_CONFIG, "w").write(
#             GCN_INFERNENCE_YAML.format(
#                 gcn_weights=pjoin(os.getcwd(), GCN_WEIGHTS, "GCN/epoch210_model.pt")
#             )
#         )
#
# rule drawsticks:
#     "Produces stick videos"
#     input:
#         skels = pjoin(DUMP_BASE, "{base}.{var}.h5"),
#         video = pjoin(VIDEO_BASE, "{base}.mp4")
#     output:
#         pjoin(DUMP_BASE, "{base}.{var}.sticks.mp4")
#     shell:
#         "python -m skelshop --ffprobe-bin {FFPROBE_BIN} drawsticks " +
#         "{input.skels} {input.video} {output}"





