"""
Run default:   `MA_DEBUG=1 PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR default`
or             `MA_DEBUG=1 PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR tcsldp_translate/ppmi_3d/pp_keybert_count/clusters.json`
or:            `(export $(cat $MA_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR default)`
ALL Combis:    `ma_cont snakemake --cores 1 -p  --directory /opt/data all --keep-going`
ALL for 1 rule `(export $(cat $MA_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR all_for --config for_rule=create_embedding)`
Plot DAG:      `PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -np --directory $MA_DATA_DIR --dag | grep -A99999 "digraph" | dot -Tsvg > dag.svg`
Get results:   `rsync -az --progress $MA_DATA_DIR etlpipelines:~/data --exclude .snakemake`
UPDATE: ` (export $(cat $MA_SELECT_ENV_FILE | xargs) && export $(cat $MA_ENV_FILE | xargs) && PYTHONPATH=$(realpath .):$PYTHONPATH snakemake --cores 1 -p --directory $MA_DATA_DIR/$MA_DATASET_NAME)`
"""

#TODO before writing about it: have a way of not having to set the PYTHONPATH as cmd-arg

import os
from string import Formatter
from functools import partial

from snakemake.io import expand

from misc_util.pretty_print import pretty_print as print
from misc_util.telegram_notifier import telegram_notify

from derive_conceptualspace.create_spaces.translate_descriptions import (
    full_translate_titles as translate_titles_base,
    full_translate_descriptions as translate_descriptions_base,
    # count_translations as count_translations_base
)
from derive_conceptualspace.extract_keywords.postprocess_candidates import (
    postprocess_candidateterms as postprocess_candidateterms_base,
)
from derive_conceptualspace.extract_keywords.keywords_main import (
    extract_candidateterms as extract_candidateterms_base,
    create_filtered_doc_cand_matrix as create_filtered_doc_cand_matrix_base,
)
from derive_conceptualspace.create_spaces.preprocess_descriptions import (
    preprocess_descriptions_full as preprocess_descriptions_base,
)
from derive_conceptualspace.create_spaces.spaces_main import (
    create_dissim_mat as create_dissim_mat_base,
)
from derive_conceptualspace.create_spaces.create_embedding import (
    create_embedding as create_embedding_base,
)
from derive_conceptualspace.semantic_directions.create_candidate_svm import (
    create_candidate_svms as create_candidate_svms_base
)
from derive_conceptualspace.pipeline import Context, cast_config

from derive_conceptualspace.settings import (
    get_setting,
    ENV_PREFIX,
    set_envvar,
    get_envvar,
    FORBIDDEN_COMBIS
)

from derive_conceptualspace.pipeline import setup_json_persister, get_jsonpersister_args
from misc_util.pretty_print import print_multicol
from derive_conceptualspace import settings

########################################################################################################################
########################################################################################################################
######### the file `kurse-beschreibungen.csv` is the absolute basis, that one I need to get and cannot create. #########
########################################################################################################################
########################################################################################################################

overwritable = lambda param, value: get_envvar(envvarname = ENV_PREFIX + "_" + param.upper().replace("-","_")) or value
flatten = lambda l: [item for sublist in l for item in sublist]

LAST_RULE = "create_candidate_svm"

def initialize_snakemake():
    if int(os.getenv(f"{ENV_PREFIX}_SNAKEMAKE_TELEGRAM", 0)):
        if not os.getenv(f"{ENV_PREFIX}_SMK_INITIALIZED"):
            print("Telegaram-Notifications ON.")
        for k, v in dict(globals()).items():
            if k.endswith("_base") and callable(v):
                globals()[k] =  telegram_notify(only_terminal=False, only_on_fail=False, log_start=True)(globals()[k])
    ctx = Context(base_dir=os.path.join(os.getcwd(), ".."), dataset_name=get_setting("DATASET_NAME", silent=True))
    os.environ[f"{ENV_PREFIX}_SMK_INITIALIZED"] = "1"
    return ctx, ctx.obj["json_persister"]

ctx, json_persister = initialize_snakemake()

def add_to_ctx(wildcards, ctx):
    for k, v in wildcards.items():
        v = cast_config(k,v)
        ctx.obj[k] = v
        set_envvar(ENV_PREFIX+"_"+k.upper().replace("-","_"), v)


def autoload_context_inputs(ctx, inputs, wildcards, params=None, input_kwargs=None):
    """
    input_kwargs could eg. be: `input_kwargs={"pp_descriptions": {"relevant_metainf": {"n_samples": "ANY"}}}` to ignore the n_samples check for ONE call.
    TO ignore it from all,
    """
    def loader(key):
        if key in ctx.autoloader_di:
            return ctx.autoloader_di[key]
        return lambda **kwargs: kwargs[key] if key in kwargs else kwargs
    input_kwargs = input_kwargs or {}
    add_to_ctx(wildcards, ctx)
    if params: add_to_ctx(params, ctx)
    for key, val in inputs.items():
        ctx.obj[key] = json_persister.load(val, key, loader=loader(key), **input_kwargs.get(key, {}))


########################################################################################################################
########################################################################################################################
########################################################################################################################

dir_struct = get_jsonpersister_args()[2]

rule trigger_default:
    input:
        ".alldone"

########################################################################################################################


generated_paths = dict(
    pp_descriptions = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/pp_descriptions.json",
    dissim_mat = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/dissim_mat_{quantification_measure}.json",
    embedding = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/{quantification_measure}_{embed_algo}_{embed_dimensions}d/embedding.json",
    candidate_terms = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/candidate_terms_{extraction_method}.json",
    postprocessed_candidates = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/postprocessed_candidates_{extraction_method}.json",
    filtered_dcm = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/filtered_dcm_{extraction_method}_{dcm_quant_measure}.json",
    clusters = "debug_{debug}/{pp_components}_{translate_policy}_minwords{min_words_per_desc}/{quantification_measure}_{embed_algo}_{embed_dimensions}d/{extraction_method}_{dcm_quant_measure}/clusters.json"
)



rule preprocess_descriptions:
    input:
        raw_descriptions = "kurse-beschreibungen.csv",
        translations = "translated_descriptions.json",
        languages = "languages.json",
        title_languages = "title_languages.json",
        title_translations = "translated_titles.json",
    output:
        generated_paths["pp_descriptions"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards, input_kwargs={"translations": {"force_overwrite": True}, "title_translations": {"force_overwrite": True}, "languages": {"force_overwrite": True}})
        descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], wildcards.pp_components, wildcards.translate_policy, ctx.obj["languages"], ctx.obj["translations"], ctx.obj["title_languages"], ctx.obj["title_translations"])
        json_persister.save("pp_descriptions.json", descriptions=descriptions, relevant_metainf=metainf)


rule preprocess_descriptions_notranslate:
    input:
        raw_descriptions = overwritable("raw_descriptions_file", "kurse-beschreibungen.csv"), #TODO add this to the schema of the rest
        languages = "languages.json",
    output:
        generated_paths["pp_descriptions"]
    threads: 1
    run:
        assert wildcards.translate_policy != "translate"
        autoload_context_inputs(ctx, input, wildcards)
        descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], wildcards.pp_components, wildcards.translate_policy, ctx.obj["languages"])
        json_persister.save("pp_descriptions.json", descriptions=descriptions, relevant_metainf=metainf)

ruleorder: preprocess_descriptions > preprocess_descriptions_notranslate


rule create_dissim_mat:
    input:
        pp_descriptions = generated_paths["pp_descriptions"]
    output:
        generated_paths["dissim_mat"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        quant_dtm, dissim_mat, metainf = create_dissim_mat_base(ctx.obj["pp_descriptions"], ctx.obj["quantification_measure"], ctx.obj["verbose"])
        json_persister.save("dissim_mat.json", quant_dtm=quant_dtm, dissim_mat=dissim_mat, relevant_metainf=metainf)


rule create_embedding:
    input:
        dissim_mat = generated_paths["dissim_mat"]
    output:
        generated_paths["embedding"]
    threads: 3
    run:
        autoload_context_inputs(ctx, input, wildcards)
        embedding = create_embedding_base(ctx.obj["dissim_mat"], ctx.obj["embed_dimensions"], ctx.obj["embed_algo"])
        json_persister.save("embedding.json", embedding=embedding)



rule extract_candidate_terms:
    input:
        pp_descriptions = generated_paths["pp_descriptions"]
    output:
        generated_paths["candidate_terms"]
    threads: 1
    params:
        faster_keybert=get_setting("FASTER_KEYBERT")
    run:
        autoload_context_inputs(ctx, input, wildcards, params)
        candidateterms, relevant_metainf = extract_candidateterms_base(ctx.obj["pp_descriptions"], ctx.obj["extraction_method"], ctx.obj["faster_keybert"], verbose=ctx.obj["verbose"])
        json_persister.save("candidate_terms.json", candidateterms=candidateterms, relevant_metainf=relevant_metainf)


rule postprocess_candidateterms:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        candidate_terms = generated_paths["candidate_terms"],
    output:
        generated_paths["postprocessed_candidates"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        postprocessed_candidates = postprocess_candidateterms_base(ctx.obj["candidate_terms"], ctx.obj["pp_descriptions"], ctx.obj["extraction_method"])
        json_persister.save("postprocessed_candidates.json", postprocessed_candidates=postprocessed_candidates)


rule create_doc_cand_matrix:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        postprocessed_candidates = generated_paths["postprocessed_candidates"],
    output:
        generated_paths["filtered_dcm"]
    threads: 1
    params:
        candidate_min_term_count = get_setting("CANDIDATE_MIN_TERM_COUNT"),
        use_ndocs_count = get_setting("CANDS_USE_NDOCS_COUNT")
    run:
        autoload_context_inputs(ctx, input, wildcards, params=params)
        filtered_dcm = create_filtered_doc_cand_matrix_base(ctx.obj["postprocessed_candidates"], ctx.obj["pp_descriptions"], min_term_count=params.candidate_min_term_count,
                                                    dcm_quant_measure=ctx.obj["dcm_quant_measure"], use_n_docs_count=params.use_ndocs_count, verbose=ctx.obj["verbose"])
        json_persister.save("filtered_dcm.json", relevant_metainf={"candidate_min_term_count": ctx.obj["candidate_min_term_count"]}, doc_term_matrix=filtered_dcm)


rule create_candidate_svm:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        filtered_dcm = generated_paths["filtered_dcm"],
        embedding = generated_paths["embedding"]
    output:
        generated_paths["clusters"]
    threads: 3
    params:
        prim_lambda = get_setting("PRIM_LAMBDA"),
        sec_lambda = get_setting("SEC_LAMBDA")
    run:
        autoload_context_inputs(ctx, input, wildcards, params=params)
        clusters, cluster_directions, decision_planes, metrics = create_candidate_svms_base(ctx.obj["filtered_dcm"], ctx.obj["embedding"], ctx.obj["pp_descriptions"],
                                                                        prim_lambda=ctx.obj["prim_lambda"], sec_lambda=ctx.obj["sec_lambda"], verbose=ctx.obj["verbose"])
        json_persister.save("clusters.json", clusters=clusters, cluster_directions=cluster_directions, decision_planes=decision_planes, metrics=metrics,
                            relevant_metainf={"prim_lambda": params.prim_lambda, "sec_lambda": params.sec_lambda})

########################################################################################################################
########################################################################################################################
########################################################################################################################

def rule_by_name(name):
    all_rules = {k: v for k, v in rules.__dict__.items() if k not in ["all", "default", "all_for", "trigger_default"]}
    return all_rules.get(name)

def expand_output(output, expand_all=False):
    set_val = (lambda name: settings.__dict__.get("ALL_" + name.upper(), get_setting(name.upper()))) if expand_all else (lambda name: get_setting(name.upper()))
    return [expand(out, **{i[1]: set_val(i[1]) for i in Formatter().parse(out) if i[1]}) for out in output]


def input_all_for(wildcards, rule_name=None):
    darule = rule_by_name(rule_name or config.get("for_rule", "preprocess_descriptions"))
    if darule:
        inputs = [i for i in flatten(expand_output(darule.output, expand_all=True)) if not any(j in i for j in FORBIDDEN_COMBIS)]
        if inputs:
            print("Running all rules for these inputs:")
            print_multicol(inputs)
        return inputs


rule all:
    input:
        partial(input_all_for, rule_name=LAST_RULE)


rule all_for:
    input:
        input_all_for


rule default:
    input:
        expand_output(rule_by_name(LAST_RULE).output)
    output:
        touch(".alldone")
