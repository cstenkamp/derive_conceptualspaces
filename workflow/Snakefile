"""
Run default:   `snakemake --cores 1 -p default`
specific file: `snakemake --cores 1 -p --directory $MA_DATA_DIR siddata/debug_True/fautcsdp_translate_minwords100/embedding_ppmi/dissim_mat.json`
ALL Combis:    `snakemake --cores 1 -p all --keep-going`
ALL for 1 rule `snakemake --cores 1 -p all_for --config for_rule=create_embedding`
from config:   `snakemake --cores 1 -p by_config --configfile ./config/derrac2015.yml --keep-going`
(if you didn't install this codebase via pip, you'll have to instead run `PYTHONPATH=$(realpath .):$PYTHONPATH snakemake ...`!)

Plot DAG:      `MA_DATASET=siddata snakemake --cores 1 -np --directory $MA_DATA_DIR --dag default | grep -A99999 "digraph" | dot -Tsvg > dag.svg`
Get results:   `rsync -az --progress grid:/net/projects/scratch/winter/valid_until_31_July_2022/cstenkamp/data/placetypes $MA_DATA_DIR --exclude .snakemake`
Get clusters : `rsync -az --progress grid:/net/projects/scratch/winter/valid_until_31_July_2022/cstenkamp/data/siddata2022 $MA_DATA_DIR  --include="*/" --include="clusters.json" --exclude="*"`
Docker in VM:  `ma_cont snakemake --cores 3 -p --directory /opt/data/siddata by_config --configfile /opt/derive_conceptualspaces/config/derrac2015_edited.yml`
                  where ma_cont = `docker run -it --rm --user $(id -u):$(id -g) --name derive_conceptualspaces_cont -v ~/data:/opt/data --env-file ~/derive_conceptualspaces/test_env.env derive_conceptualspaces`
                  (so there's a bug that you have to specify `--directory` which includes the dataset-name but whatever)
SGE-Grid (IKW) `snakemake -p by_config --configfile $HOME/derive_conceptualspaces/config/derrac2015_edited.yml --directory $DATAPATH --profile $HOME/derive_conceptualspaces/workflow/ikw_grid/sge` inside an .sge-file (see ./sge)
"""
#TODO: Job Groups! See https://snakemake.readthedocs.io/en/stable/executing/cluster.html
#TODO: the rules that have multiprocessing (currently: create_candidate_svm) should require more CPUs!

import itertools
import os
import warnings
from builtins import all
from functools import partial
from string import Formatter
import sys
from pathlib import Path
from os.path import abspath, join, dirname, splitext
import socket

from dotenv.main import load_dotenv
from snakemake import rules
from snakemake.io import expand, touch

from derive_conceptualspace import settings
from derive_conceptualspace.create_spaces.translate_descriptions import (
    create_languages_file as create_languages_file_base,
)
from derive_conceptualspace.create_spaces.create_embedding import (
    create_embedding as create_embedding_base,
)
from derive_conceptualspace.create_spaces.preprocess_descriptions import (
    preprocess_descriptions_full as preprocess_descriptions_base,
)
from derive_conceptualspace.create_spaces.spaces_main import (
    create_dissim_mat as create_dissim_mat_base,
)
from derive_conceptualspace.extract_keywords.keywords_main import (
    extract_candidateterms as extract_candidateterms_base,
    create_filtered_doc_cand_matrix as create_filtered_doc_cand_matrix_base,
)
from derive_conceptualspace.extract_keywords.postprocess_candidates import (
    postprocess_candidateterms as postprocess_candidateterms_base,
)
from derive_conceptualspace.pipeline import CustomContext, SnakeContext, load_lang_translate_files, apply_dotenv_vars, generated_paths
from derive_conceptualspace.semantic_directions.create_candidate_svm import (
    create_candidate_svms as create_candidate_svms_base
)
from derive_conceptualspace.settings import (
    ENV_PREFIX,
    standardize_config,
    standardize_config_name,
    FORBIDDEN_COMBIS,
)
from derive_conceptualspace.util.interruptible_funcs import InterruptibleLoad
from misc_util.pretty_print import pretty_print as print, print_multicol
from misc_util.telegram_notifier import telegram_notify, send_message

flatten = lambda l: [item for sublist in l for item in sublist]

LAST_RULE = "create_candidate_svm"
NONDEFAULT_RULES = ["all", "default", "all_for", "by_config"]

#on the cluster, I sometimes get this error: https://github.com/snakemake/snakemake/issues/1244

########################################################################################################################
########################################################################################################################
########### the file `raw_descriptions.csv` is the absolute basis, that one I need to get and cannot create. ###########
########################################################################################################################
########################################################################################################################

def initialize_snakemake():
    if os.getenv(f"{ENV_PREFIX}_SELECT_ENV_FILE"):
        load_dotenv(os.getenv(f"{ENV_PREFIX}_SELECT_ENV_FILE"))
    apply_dotenv_vars()
    warnings.formatwarning = lambda message, category, filename, lineno, file=None, line=None: f"{filename.replace(abspath(join(dirname(__file__),'..')) + os.sep,'')}:{lineno}: {category.__name__}: {message}\n"
    if int(os.getenv(f"{ENV_PREFIX}_SNAKEMAKE_TELEGRAM", 0)):
        if not os.getenv(f"{ENV_PREFIX}_SMK_INITIALIZED"):
            print("Telegaram-Notifications ON.")
        for k, v in dict(globals()).items():
            if k.endswith("_base") and callable(v):
                globals()[k] =  telegram_notify(only_terminal=False, only_on_fail=False, log_start=True)(globals()[k])
    if any(i in sys.argv for i in NONDEFAULT_RULES):
        os.environ[f"{ENV_PREFIX}_SMK_NONDEFAULTRULE"] = [i for i in sys.argv if i in NONDEFAULT_RULES][0]
    ctx = CustomContext(SnakeContext())
    os.environ[f"{ENV_PREFIX}_SMK_INITIALIZED"] = "1"
    return ctx

def autoload_context_inputs(ctx, inputs, wildcards, params=None, input_kwargs=None, silents=None):
    #input_kwargs could eg. be: `input_kwargs={"pp_descriptions": {"metainf": {"n_samples": "ANY"}}}` to ignore the n_samples check for ONE call.
    def loader(key):
        if key in ctx.autoloader_di:
            return ctx.autoloader_di[key]
        return lambda **kwargs: kwargs[key] if key in kwargs else kwargs
    def add_to_ctx(wildcards, ctx):
        for k, v in wildcards.items():
            k, v = standardize_config(k,v)
            ctx.set_config(k,v,"smk_wildcard")
    input_kwargs = input_kwargs or {}
    add_to_ctx(wildcards, ctx)
    if params: add_to_ctx(params, ctx)
    ctx.init_context(load_envfile=True, load_conffile=False)  #after this point, new-env-vars are no considered anymore!
    for key, val in inputs.items():
        ctx.obj[key] = ctx.p.load(val, key, loader=loader(key), **input_kwargs.get(key, {}), silent=key in (silents or []))
    ctx.print_important_settings()
    # TODO overhaul: don't I need to set envvars in snakemake? -> maybe call confirm_config() here which sets all the envvars?

ctx = initialize_snakemake()



########################################################################################################################
########################################################################################################################
########################################################################################################################

# from workflow.func_arg import func
# rule test_rule:
#     output:
#         touch("success.file")
#     run:
#         func()

# rule get_languages:
#     input:
#         raw_descriptions = "{dataset}/"+ctx.get_config("raw_descriptions_file"),
#     output:
#         languages        = "{dataset}/"+ctx.get_config("languages_file"),
#         title_languages  = "{dataset}/"+ctx.get_config("title_languages_file"),
#     threads: 1
#     run:
#         autoload_context_inputs(ctx, input, wildcards)
#         create_languages_file_base(ctx.get_config("dataset")+os.sep+ctx.get_config("languages_file"), "languages", "Beschreibung", ctx.p, ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], declare_silent=True)
#         create_languages_file_base(ctx.get_config("dataset")+os.sep+ctx.get_config("title_languages_file"), "title_languages", "Name", ctx.p, ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], declare_silent=True)
#TODO check what's new for this


########################################################################################################################
#preprocess-descriptions (multiple rules bc multiple usecases)

def raw_descriptions_input(wildcards):
    # I cannot do `raw_descriptions = "{dataset}/"+ctx.get_config("raw_descriptions_file")` in the input-block of snakemake, because
    # the inputs are executed before eg. the dataset-class is loaded and I would lock the config too early. So https://bioinformatics.stackexchange.com/a/17828/10588
    try:
        return next((Path(".")/wildcards.dataset).glob("raw_descriptions.*"))
    except StopIteration as err:
        raise Exception("No raw_descriptions file found") from err

rule preprocess_descriptions:
    input:
        raw_descriptions         = raw_descriptions_input,
        description_languages    = "{dataset}/description_languages.json",
        title_languages          = "{dataset}/title_languages.json",
        # subtitle_languages       = "{dataset}/subtitle_languages.json",
        description_translations = "{dataset}/description_translations_{language}.json",
        title_translations       = "{dataset}/title_translations_{language}.json",
        # subtitle_translations    = "{dataset}/title_translations.json",
    output:
        generated_paths["pp_descriptions"]
    threads: 1
    resources:
        mem_mb = 2048
    run:
        autoload_context_inputs(ctx, {"raw_descriptions": input["raw_descriptions"]}, wildcards)
        languages, translations = load_lang_translate_files(ctx, ctx.p, ctx.get_config("pp_components"))
        descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], ctx.get_config("pp_components"),
                                                             ctx.get_config("language"), ctx.get_config("translate_policy"), languages, translations)
        ctx.p.save("pp_descriptions.json", descriptions=descriptions, metainf=metainf)



rule preprocess_descriptions_notranslate:
    input:
        raw_descriptions       = raw_descriptions_input,
        description_languages  = "{dataset}/description_languages.json",
        title_languages        = "{dataset}/title_languages.json",
        # subtitle_languages     = "{dataset}/subtitle_languages.json",
    output:
        generated_paths["pp_descriptions"]
    threads: 1
    run:
        assert ctx.get_config("translate_policy") != "translate", "This rule shouldn't be triggered if translate-policy == translate!!"
        autoload_context_inputs(ctx, {"raw_descriptions": input["raw_descriptions"]}, wildcards)
        languages, _ = load_lang_translate_files(ctx, ctx.p, ctx.get_config("pp_components"))
        descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], ctx.get_config("pp_components"),
                                                             ctx.get_config("language"), ctx.get_config("translate_policy"), languages)
        ctx.p.save("pp_descriptions.json", descriptions=descriptions, metainf=metainf)


rule preprocess_descriptions_singlelanguage:
    input:
        raw_descriptions = raw_descriptions_input,
    output:
        generated_paths["pp_descriptions"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        assert ctx.has_config("all_descriptions_lang") and ctx.get_config("all_descriptions_lang"), "This rule should only be triggered if all_descriptions_lang is set for the dataset!"
        descriptions, metainf = preprocess_descriptions_base(ctx.obj["raw_descriptions"], ctx.obj["dataset_class"], ctx.get_config("pp_components"),
                                                             ctx.get_config("language"), ctx.get_config("translate_policy"), ctx.get_config("all_descriptions_lang"))
        ctx.p.save("pp_descriptions.json", descriptions=descriptions, metainf=metainf)

ruleorder: preprocess_descriptions > preprocess_descriptions_notranslate > preprocess_descriptions_singlelanguage

########################################################################################################################

rule create_dissim_mat:
    input:
        pp_descriptions = generated_paths["pp_descriptions"]
    output:
        generated_paths["dissim_mat"]
    threads: 3
    resources:
        mem_mb = 29*1024, #TODO make also dependent on the size of the dataset ("input" is also allowed as arg)
        pe = "3"   #I want this to run with 2 processes
    run:
        autoload_context_inputs(ctx, input, wildcards)
        with InterruptibleLoad(ctx, "dissim_mat.json", metainf_ignorevarnames=["is_dissim"], loader=(lambda **kw: [kw["dissim_mat"]])) as mgr:
            quant_dtm, dissim_mat, metainf = create_dissim_mat_base(ctx.obj["pp_descriptions"], ctx.get_config("quantification_measure"), ctx.get_config("verbose"), **mgr.kwargs)
        res = mgr.save(quant_dtm=quant_dtm, dissim_mat=dissim_mat, metainf=metainf)
        if res == 1: exit(1)
        # quant_dtm, dissim_mat, metainf = create_dissim_mat_base(ctx.obj["pp_descriptions"], ctx.get_config("quantification_measure"), verbose=ctx.get_config("verbose"))
        # ctx.p.save("dissim_mat.json", quant_dtm=quant_dtm, dissim_mat=dissim_mat, metainf=metainf)


rule create_embedding:
    input:
        pp_descriptions = generated_paths["pp_descriptions"], #silent
        dissim_mat = generated_paths["dissim_mat"]
    output:
        generated_paths["embedding"]
    threads: 3
    run:
        autoload_context_inputs(ctx, input, wildcards, silents=["pp_descriptions"])
        embedding = create_embedding_base(ctx.obj["dissim_mat"], ctx.get_config("embed_dimensions"), ctx.get_config("embed_algo"), verbose=ctx.get_config("verbose"), pp_descriptions=ctx.obj["pp_descriptions"])
        ctx.p.save("embedding.json", embedding=embedding)



rule extract_candidate_terms:
    input:
        pp_descriptions = generated_paths["pp_descriptions"]
    output:
        generated_paths["candidate_terms"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        with InterruptibleLoad(ctx, "candidate_terms.json", metainf_countervarnames=["n_candidateterms", "n_immediateworking", "n_fixed", "n_errs"]) as mgr:
            candidate_terms, metainf = extract_candidateterms_base(ctx.obj["pp_descriptions"], ctx.get_config("extraction_method"), ctx.get_config("max_ngram"), verbose=ctx.get_config("verbose"), **mgr.kwargs)
        res = mgr.save(candidate_terms=candidate_terms, metainf=metainf)
        if res == 1: exit(1)


rule postprocess_candidateterms:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        candidate_terms = generated_paths["candidate_terms"],
    output:
        generated_paths["postprocessed_candidates"]
    threads: 1
    run:
        autoload_context_inputs(ctx, input, wildcards)
        postprocessed_candidates, changeds = postprocess_candidateterms_base(ctx.obj["candidate_terms"], ctx.obj["pp_descriptions"], ctx.get_config("extraction_method"))
        ctx.p.save("postprocessed_candidates.json", postprocessed_candidates=postprocessed_candidates, changeds=changeds)


rule create_doc_cand_matrix:
    input:
        pp_descriptions = generated_paths["pp_descriptions"],
        postprocessed_candidates = generated_paths["postprocessed_candidates"],
    output:
        generated_paths["filtered_dcm"]
    threads: 1
    resources:
        mem_mb=lambda wildcards: 24 * 1024 if wildcards.dcm_quant_measure == "ppmi" else 8*1024  #TODO make also dependent on the size of the dataset ("input" is also allowed as arg)
    run:
        autoload_context_inputs(ctx, input, wildcards)
        filtered_dcm = create_filtered_doc_cand_matrix_base(ctx.obj["postprocessed_candidates"], ctx.obj["pp_descriptions"], min_term_count=ctx.get_config("candidate_min_term_count"),
                                                    dcm_quant_measure=ctx.get_config("dcm_quant_measure"), use_n_docs_count=ctx.get_config("cands_use_ndocs_count"), verbose=ctx.get_config("verbose"))
        ctx.p.save("filtered_dcm.json", doc_term_matrix=filtered_dcm)


rule create_candidate_svm:
    input:
        pp_descriptions = generated_paths["pp_descriptions"], #silent
        filtered_dcm = generated_paths["filtered_dcm"],
        embedding = generated_paths["embedding"]
    output:
        generated_paths["clusters"]
    threads: 4
    resources:
        mem_mb=32*1024,
        pe="4-6"
    run:
        autoload_context_inputs(ctx, input, wildcards, silents=["pp_descriptions"])
        with InterruptibleLoad(ctx, "clusters.json", loader=lambda x:x) as mgr:
            quants_s, decision_planes, metrics, metainf = create_candidate_svms_base(ctx.obj["filtered_dcm"], ctx.obj["embedding"], ctx.obj["pp_descriptions"], verbose=ctx.get_config("verbose"), **mgr.kwargs)
        res = mgr.save(quants_s=quants_s, decision_planes=decision_planes, metrics=metrics, metainf=metainf)
        if res == 1: exit(1)

########################################################################################################################
########################################################################################################################
########################################################################################################################

def rule_by_name(name): # must be after all other rules!
    all_rules = {k: v for k, v in rules.__dict__.items() if k not in NONDEFAULT_RULES}
    return all_rules.get(name)
    #we may call with `--allowed-rules " ".join(k for k in rules.__dict__ if k not in NONDEFAULT_RULES)` (there's also `--omit-from`)

def expand_output(output, expand_all=False, overwriter_di=None, caller=None):
    if not (any(i in sys.argv for i in NONDEFAULT_RULES) or os.getenv(f"{ENV_PREFIX}_SMK_NONDEFAULTRULE")):
        return ["Not_An_Input"] #otherwise other rules want to checkout the rules with this
    #so normally, if caller=="default" and settings.SMK_DEFAULT_CONFIG, we want load_conffile=True, HOWEVER in cluster-settings it may be caller=="default" even though it really shouldn't.
    if os.getenv(f"{ENV_PREFIX}_SMK_NONDEFAULTRULE", caller) != caller: caller = os.getenv(f"{ENV_PREFIX}_SMK_NONDEFAULTRULE")
    ctx.init_context(load_envfile=True, load_conffile=(caller=="default" and settings.SMK_DEFAULT_CONFIG))  #after this point, new env-vars are not considered
    set_val = bkp = (lambda name: settings.__dict__.get("ALL_"+standardize_config_name(name), ctx.get_config(standardize_config_name(name), silent=True))) \
                    if expand_all else (lambda name: ctx.get_config(standardize_config_name(name), silent=True))
    if overwriter_di:
        set_val = lambda name: overwriter_di[name] if name in overwriter_di else bkp(name)
    return [expand(out, **{i[1]: set_val(i[1]) for i in Formatter().parse(out) if i[1]}) for out in output]


def input_all_for(wildcards, rule_name=None):
    darule = rule_by_name(rule_name or config.get("for_rule", "preprocess_descriptions"))
    if darule:
        inputs = [i for i in flatten(expand_output(darule.output, expand_all=True)) if not any(j in i for j in FORBIDDEN_COMBIS)]
        if inputs:
            print("Running all rules for these inputs:")
            print_multicol(inputs)
        return inputs

def notify_done(inpt):
    print(f"Created all of these: {inpt}")
    if int(os.getenv(f"{ENV_PREFIX}_SNAKEMAKE_TELEGRAM", "0")):
        if "TELEGRAM_MY_CHAT_ID" in os.environ and "TELEGRAM_BOT_TOKEN" in os.environ:
            send_message(f"Snakemake on {socket.gethostname()} finished.", os.environ["TELEGRAM_MY_CHAT_ID"])

rule all:
    input:
        partial(input_all_for, rule_name=LAST_RULE)
    run:
        notify_done(input)


rule all_for:
    input:
        input_all_for
    run:
        notify_done(input)

rule default:
    input:
        expand_output(rule_by_name(LAST_RULE).output, caller="default")
    run:
        notify_done(input)


def set_envvars_generate_input_from_config(cnf):
    """the by_config rule which allows to provide a yaml with a configuration. This reads out this configuration and
    sets the correct environment-variables for the demanded settings and lists the required output-files. It is possible
    to generate multiple outputs, so those settings that are reflected in the filenames can have lists as value, such that
    all combination of these will be tried, however those settings that must be set as env-vars cannot be lists and must
    be equal for all combinations."""
    if os.getenv(f"{ENV_PREFIX}_SMK_NONDEFAULTRULE") != "by_config":
        return ["Not_An_Input"]
    if config.get("__perdataset__"):
        if config["__perdataset__"].get(ctx.get_config("dataset"),{}):
            config.update(config.get("__perdataset__",{}).get(ctx.get_config("dataset"),{}))
        del config["__perdataset__"]
    lkeys, lvals = zip(*[(k,v) for k,v in cnf.items() if isinstance(v, list)])
    othervals = {k: v for k,v in cnf.items() if k not in lkeys}
    configs_in_filename = flatten([[i[1] for i in Formatter().parse(out) if i[1]] for out in rule_by_name(LAST_RULE).output])
    if not all(i in configs_in_filename for i in lkeys):
        raise ValueError(f"You can only have multiple values for configs that are reflected in the filename, and these are not: {', '.join(i for i in lkeys if i not in configs_in_filename)}")
    for k, v in othervals.items():
        if k not in configs_in_filename:
            os.environ[ENV_PREFIX+"_CONF_FORCE_"+standardize_config_name(k)] = str(v)
    outputs = []
    for multival_comb in itertools.product(*lvals):
        all_vals = {**dict(zip(lkeys,multival_comb)), **othervals}
        outputs.append(expand_output(rule_by_name(LAST_RULE).output, overwriter_di=all_vals))
    return outputs


rule by_config:
    input:
        set_envvars_generate_input_from_config(config)
    run:
        notify_done(input)